# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.nn.functional as F

import trimesh
from skimage import measure

from ...modules import sparse as sp
from .encoder import SparseSDFEncoder
from .decoder import SparseSDFDecoder
# VQVAEä¸éœ€è¦DiagonalGaussianDistributionï¼ˆç§»é™¤VAEçš„é«˜æ–¯é‡‡æ ·æœºåˆ¶ï¼‰


class SparseVectorQuantizer(nn.Module):
    """
    ç¨€ç–å¼ é‡çš„ Vector Quantizer
    æ”¯æŒä¸¤ç§ç æœ¬æ›´æ–°æ¨¡å¼ï¼š
    1. æ¢¯åº¦æ›´æ–°æ¨¡å¼ (use_ema_update=False): é€šè¿‡åå‘ä¼ æ’­æ›´æ–°ç æœ¬
    2. EMAæ›´æ–°æ¨¡å¼ (use_ema_update=True): é€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡ç»Ÿè®¡æ›´æ–°ç æœ¬
    """
    def __init__(self, num_embeddings: int = 8192, embedding_dim: int = 64, beta: float = 0.25,
                 use_ema_update: bool = False, decay: float = 0.99, epsilon: float = 1e-5):
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.beta = beta
        self.use_ema_update = use_ema_update
        self.decay = decay
        self.epsilon = epsilon
        
        # ç æœ¬åµŒå…¥
        self.embeddings = nn.Embedding(self.num_embeddings, self.embedding_dim)
        self.embeddings.weight.data.normal_(mean=0.0, std=1.0)
        
        # æ ¹æ®æ›´æ–°æ¨¡å¼è®¾ç½®requires_gradå’Œåˆå§‹åŒ–buffer
        if use_ema_update:
            # EMAæ¨¡å¼ï¼šç¦ç”¨æ¢¯åº¦ï¼Œæ³¨å†Œç»Ÿè®¡buffer
            self.embeddings.weight.requires_grad = False
            # ğŸ”§ ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å…ˆéªŒï¼ˆä¼ªè®¡æ•°ï¼‰åˆå§‹åŒ–ï¼Œé¿å…æœªä½¿ç”¨ç æœ¬åå¡Œä¸º0
            # ema_cluster_size = 1: æ¯ä¸ªç æœ¬æœ‰1æ¬¡å…ˆéªŒè®¡æ•°
            # ema_w = embedding: å½“c=1æ—¶ï¼Œe=w/c=embeddingï¼Œä¿æŒåˆå§‹åˆ†å¸ƒ
            # è¿™æ ·æœªä½¿ç”¨çš„ç æœ¬ä¼šä¿æŒåŸå€¼ï¼Œè€Œä¸æ˜¯è¡°å‡åˆ°0å‘é‡
            self.register_buffer('ema_cluster_size', torch.ones(num_embeddings))
            self.register_buffer('ema_w', self.embeddings.weight.data.clone())
        # else: æ¢¯åº¦æ¨¡å¼ä¿æŒé»˜è®¤requires_grad=True
    

    def forward(self, z: sp.SparseTensor, only_return_indices: bool = False):
        """
        Args:
            z: SparseTensorï¼Œfeats shape ä¸º [N, embedding_dim]ï¼ŒN æ˜¯æ¿€æ´»ä½“ç´ æ•°é‡
            only_return_indices: æ˜¯å¦åªè¿”å› indices
        Returns:
            å¦‚æœ only_return_indices=True: è¿”å› indices çš„ SparseTensor
            å¦åˆ™: è¿”å› (quantized, vq_loss, commitment_loss, encoding_indices)
            æ³¨æ„ï¼šå½“use_ema_update=Trueæ—¶ï¼Œvq_lossä¸ºNone
        """
        print(f"\n[DEBUG VQ] Input z.feats: shape={z.feats.shape}, min={z.feats.min().item():.6f}, max={z.feats.max().item():.6f}, mean={z.feats.mean().item():.6f}, std={z.feats.std().item():.6f}")
        print(f"[DEBUG VQ] Codebook: min={self.embeddings.weight.min().item():.6f}, max={self.embeddings.weight.max().item():.6f}, mean={self.embeddings.weight.mean().item():.6f}, std={self.embeddings.weight.std().item():.6f}")
        print(f"[DEBUG VQ] Codebook requires_grad: {self.embeddings.weight.requires_grad}, use_ema_update: {self.use_ema_update}")
        
        # æ£€æŸ¥ç æœ¬æ˜¯å¦æœ‰å¼‚å¸¸ï¼ˆå¤ªå¤šé›¶å‘é‡ï¼‰
        codebook_norms = torch.norm(self.embeddings.weight, dim=1)  # [num_embeddings]
        zero_codes = (codebook_norms < 0.01).sum().item()
        print(f"[DEBUG VQ] Codebook norms: min={codebook_norms.min().item():.6f}, max={codebook_norms.max().item():.6f}, mean={codebook_norms.mean().item():.6f}")
        print(f"[DEBUG VQ] Near-zero codes (norm<0.01): {zero_codes}/{self.num_embeddings}")
        
        # z.feats: [N, embedding_dim]
        z_flatten = z.feats  # [N, embedding_dim]
        
        # è®¡ç®—è·ç¦»å¹¶æ‰¾åˆ°æœ€è¿‘çš„ codebook entry
        distances = torch.cdist(z_flatten, self.embeddings.weight)  # [N, num_embeddings]
        print(f"[DEBUG VQ] Distances: min={distances.min().item():.6f}, max={distances.max().item():.6f}, mean={distances.mean().item():.6f}")
        
        # ç»Ÿè®¡æœ€å°è·ç¦»çš„åˆ†å¸ƒ
        min_distances = distances.min(dim=1)[0]  # [N]
        print(f"[DEBUG VQ] Min distances: mean={min_distances.mean().item():.6f}, std={min_distances.std().item():.6f}, median={min_distances.median().item():.6f}")
        
        encoding_indices = torch.argmin(distances, dim=1)  # [N]
        unique_codes = torch.unique(encoding_indices)
        print(f"[DEBUG VQ] Encoding indices: unique codes used={len(unique_codes)}/{self.num_embeddings}")
        
        # ç»Ÿè®¡æ¯ä¸ªç æœ¬è¢«ä½¿ç”¨çš„æ¬¡æ•°
        if len(unique_codes) < 100:  # åªåœ¨æ¿€æ´»å°‘æ—¶æ‰“å°
            counts = torch.bincount(encoding_indices, minlength=self.num_embeddings)
            used_counts = counts[counts > 0]
            print(f"[DEBUG VQ] Usage distribution: min={used_counts.min().item()}, max={used_counts.max().item()}, mean={used_counts.float().mean().item():.1f}")
        
        if only_return_indices:
            # è¿”å› indices ä½œä¸º SparseTensorï¼Œä¿æŒåŸå§‹åæ ‡
            result = z.replace(encoding_indices.unsqueeze(-1).float())
            return result
        
        # é‡åŒ–
        quantized_feats = self.embeddings(encoding_indices)  # [N, embedding_dim]
        print(f"[DEBUG VQ] Quantized feats: min={quantized_feats.min().item():.6f}, max={quantized_feats.max().item():.6f}, mean={quantized_feats.mean().item():.6f}")
        
        # è®¡ç®—commitment lossï¼ˆä¸¤ç§æ¨¡å¼éƒ½éœ€è¦ï¼‰
        commitment_loss = F.mse_loss(z_flatten, quantized_feats.detach())
        
        # æ ¹æ®æ›´æ–°æ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
        if self.use_ema_update:
            # EMAæ¨¡å¼ï¼šåœ¨è®­ç»ƒæ—¶è°ƒç”¨EMAæ›´æ–°
            if self.training:
                self._update_ema(encoding_indices, z_flatten)
            vq_loss = None  # EMAæ¨¡å¼ä¸éœ€è¦vq_loss
            print(f"[DEBUG VQ] EMA mode - Commitment Loss: {commitment_loss.item():.6f}, VQ Loss: None")
        else:
            # æ¢¯åº¦æ¨¡å¼ï¼šè®¡ç®—vq_lossç”¨äºåå‘ä¼ æ’­
            vq_loss = F.mse_loss(quantized_feats, z_flatten.detach())
            print(f"[DEBUG VQ] Gradient mode - VQ Loss: {vq_loss.item():.6f}, Commitment Loss: {commitment_loss.item():.6f}")
            print(f"[DEBUG VQ] VQ Loss requires_grad: {vq_loss.requires_grad}, Commitment Loss requires_grad: {commitment_loss.requires_grad}")
        
        # Straight-through estimator
        quantized_feats = z_flatten + (quantized_feats - z_flatten).detach()
        
        # åˆ›å»ºæ–°çš„ SparseTensor
        quantized = z.replace(quantized_feats)
        encoding_indices_st = z.replace(encoding_indices.unsqueeze(-1).float())
        
        print(f"[DEBUG VQ] Output quantized feats: min={quantized.feats.min().item():.6f}, max={quantized.feats.max().item():.6f}, requires_grad={quantized.feats.requires_grad}\n")
        
        return quantized, vq_loss, commitment_loss, encoding_indices_st
    
    @torch.no_grad()
    def _update_ema(self, encoding_indices, z_flatten):
        """
        ä½¿ç”¨EMAæ›´æ–°ç æœ¬ï¼ˆä»…åœ¨use_ema_update=Trueæ—¶è°ƒç”¨ï¼‰
        
        Args:
            encoding_indices: åˆ†é…çš„ç æœ¬ç´¢å¼• [N]
            z_flatten: encoderè¾“å‡ºçš„ç‰¹å¾å‘é‡ [N, embedding_dim]
        """
        print(f"[DEBUG EMA] === Starting EMA Update ===")
        print(f"[DEBUG EMA] Input z_flatten: shape={z_flatten.shape}, min={z_flatten.min().item():.6f}, max={z_flatten.max().item():.6f}, mean={z_flatten.mean().item():.6f}, std={z_flatten.std().item():.6f}")
        print(f"[DEBUG EMA] Encoding indices: shape={encoding_indices.shape}, unique codes={len(torch.unique(encoding_indices))}/{self.num_embeddings}")
        
        # è®¡ç®—one-hotç¼–ç 
        encodings = F.one_hot(encoding_indices, self.num_embeddings).float()  # [N, num_embeddings]
        print(f"[DEBUG EMA] One-hot encodings: shape={encodings.shape}, sum={encodings.sum().item():.1f}")
        
        # æ£€æŸ¥å½“å‰EMAçŠ¶æ€
        print(f"[DEBUG EMA] OLD ema_cluster_size: sum={self.ema_cluster_size.sum().item():.1f}, min={self.ema_cluster_size.min().item():.6f}, max={self.ema_cluster_size.max().item():.6f}")
        print(f"[DEBUG EMA] OLD ema_w: min={self.ema_w.min().item():.6f}, max={self.ema_w.max().item():.6f}, mean={self.ema_w.mean().item():.6f}")
        print(f"[DEBUG EMA] OLD embeddings: min={self.embeddings.weight.data.min().item():.6f}, max={self.embeddings.weight.data.max().item():.6f}, mean={self.embeddings.weight.data.mean().item():.6f}")
        
        # EMAæ›´æ–°ç»Ÿè®¡é‡
        batch_cluster_size = encodings.sum(0)  # [num_embeddings]
        print(f"[DEBUG EMA] Batch cluster size: sum={batch_cluster_size.sum().item():.1f}, nonzero={(batch_cluster_size > 0).sum().item()}/{self.num_embeddings}")
        
        new_cluster_size = self.decay * self.ema_cluster_size + (1 - self.decay) * batch_cluster_size
        print(f"[DEBUG EMA] NEW cluster_size: sum={new_cluster_size.sum().item():.1f}, min={new_cluster_size.min().item():.6f}, max={new_cluster_size.max().item():.6f}")
        
        # è®¡ç®—batchçš„åŠ æƒç‰¹å¾å’Œ
        batch_w = encodings.t() @ z_flatten  # [num_embeddings, embedding_dim]
        print(f"[DEBUG EMA] Batch_w (encodings.t() @ z_flatten): shape={batch_w.shape}, min={batch_w.min().item():.6f}, max={batch_w.max().item():.6f}, mean={batch_w.mean().item():.6f}")
        
        new_w = self.decay * self.ema_w + (1 - self.decay) * batch_w
        print(f"[DEBUG EMA] NEW ema_w: min={new_w.min().item():.6f}, max={new_w.max().item():.6f}, mean={new_w.mean().item():.6f}")
        
        # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼ˆé¿å…æŸäº›ç æœ¬ä»æœªè¢«ä½¿ç”¨ï¼‰
        n = new_cluster_size.sum()
        print(f"[DEBUG EMA] Total cluster size n: {n.item():.1f}")
        
        smoothed_cluster_size = (
            (new_cluster_size + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n
        )
        print(f"[DEBUG EMA] Smoothed cluster size: min={smoothed_cluster_size.min().item():.6f}, max={smoothed_cluster_size.max().item():.6f}, mean={smoothed_cluster_size.mean().item():.6f}")
        
        # æ›´æ–°ç æœ¬å‘é‡ï¼ˆæ‰€æœ‰ç æœ¬ï¼ŒåŒ…æ‹¬æœªä½¿ç”¨çš„ï¼‰
        new_embeddings = new_w / (smoothed_cluster_size.unsqueeze(1) + 1e-7)
        print(f"[DEBUG EMA] NEW embeddings (all codes): min={new_embeddings.min().item():.6f}, max={new_embeddings.max().item():.6f}, mean={new_embeddings.mean().item():.6f}, std={new_embeddings.std().item():.6f}")
        
        # ç»Ÿè®¡å®é™…ä½¿ç”¨çš„ç æœ¬æ•°é‡ï¼ˆå»é™¤å…ˆéªŒè®¡æ•°å½±å“ï¼‰
        used_codes = (new_cluster_size > 1.5).sum().item()  # > 1.5è¡¨ç¤ºé™¤äº†å…ˆéªŒ1æ¬¡å¤–ï¼Œå®é™…è¢«ä½¿ç”¨è¿‡
        print(f"[DEBUG EMA] Actually used codes: {used_codes}/{self.num_embeddings} (cluster_size > 1.5)")
        
        self.embeddings.weight.data.copy_(new_embeddings)
        
        # æ›´æ–°buffer
        self.ema_cluster_size.copy_(new_cluster_size)
        self.ema_w.copy_(new_w)
        
        print(f"[DEBUG EMA] === EMA Update Complete ===\n")


class SparseSDFVQVAE(nn.Module):
    """
    Direct3D-S2 çš„ VQVAE ç‰ˆæœ¬
    ä¸¥æ ¼éµå¾ª SparseSDFVAE çš„ç»“æ„ï¼Œåªæ›¿æ¢ VQ éƒ¨åˆ†
    """
    def __init__(self, *,
                 embed_dim: int = None,
                 latent_channels: int = None,  # åˆ«åï¼Œå…¼å®¹æ—§é…ç½®
                 resolution: int = 64,
                 model_channels_encoder: int = None,
                 model_channels_decoder: int = None,
                 model_channels: int = None,  # åˆ«åï¼Œå…¼å®¹æ—§é…ç½®
                 num_blocks_encoder: int = None,
                 num_blocks_decoder: int = None,
                 num_blocks: int = None,  # åˆ«åï¼Œå…¼å®¹æ—§é…ç½®
                 num_heads_encoder: int = None,
                 num_heads_decoder: int = None,
                 num_heads: int = None,  # åˆ«åï¼Œå…¼å®¹æ—§é…ç½®
                 num_head_channels_encoder: int = 64,
                 num_head_channels_decoder: int = 64,
                 num_head_channels: int = None,  # åˆ«åï¼Œå…¼å®¹æ—§é…ç½®
                 out_channels: int = 1,
                 use_fp16: bool = False,
                 use_checkpoint: bool = False,
                 chunk_size: int = 1,
                 latents_scale: float = 1.0,
                 latents_shift: float = 0.0,
                 num_embeddings: int = 8192,
                 use_ema_update: bool = False,  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨EMAæ›´æ–°ç æœ¬
                 vq_decay: float = 0.99,        # æ–°å¢ï¼šEMAè¡°å‡ç‡
                 vq_epsilon: float = 1e-5,      # æ–°å¢ï¼šæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°
                 mlp_ratio: float = 4,
                 attn_mode: str = "swin",
                 window_size: int = 8,
                 pe_mode: str = "ape",
                 qk_rms_norm: bool = False,
                 representation_config: dict = None):

        super().__init__()
        
        # å¤„ç†å‚æ•°åˆ«åï¼ˆå…¼å®¹æ—§é…ç½®æ–‡ä»¶ï¼‰
        if latent_channels is not None and embed_dim is None:
            embed_dim = latent_channels
        if embed_dim is None:
            embed_dim = 0
            
        if model_channels is not None:
            if model_channels_encoder is None:
                model_channels_encoder = model_channels
            if model_channels_decoder is None:
                model_channels_decoder = model_channels
        if model_channels_encoder is None:
            model_channels_encoder = 512
        if model_channels_decoder is None:
            model_channels_decoder = 512
            
        if num_blocks is not None:
            if num_blocks_encoder is None:
                num_blocks_encoder = num_blocks
            if num_blocks_decoder is None:
                num_blocks_decoder = num_blocks
        if num_blocks_encoder is None:
            num_blocks_encoder = 4
        if num_blocks_decoder is None:
            num_blocks_decoder = 4
            
        if num_heads is not None:
            if num_heads_encoder is None:
                num_heads_encoder = num_heads
            if num_heads_decoder is None:
                num_heads_decoder = num_heads
        if num_heads_encoder is None:
            num_heads_encoder = 8
        if num_heads_decoder is None:
            num_heads_decoder = 8
            
        if num_head_channels is not None:
            num_head_channels_encoder = num_head_channels
            num_head_channels_decoder = num_head_channels

        self.use_checkpoint = use_checkpoint
        self.resolution = resolution
        self.latents_scale = latents_scale
        self.latents_shift = latents_shift

        self.encoder = SparseSDFEncoder(
            resolution=resolution,
            in_channels=model_channels_encoder,
            model_channels=model_channels_encoder,
            latent_channels=embed_dim,
            num_blocks=num_blocks_encoder,
            num_heads=num_heads_encoder,
            num_head_channels=num_head_channels_encoder,
            mlp_ratio=mlp_ratio,
            attn_mode=attn_mode,
            window_size=window_size,
            pe_mode=pe_mode,
            use_fp16=use_fp16,
            use_checkpoint=use_checkpoint,
            qk_rms_norm=qk_rms_norm,
        )

        self.decoder = SparseSDFDecoder(
            resolution=resolution,
            model_channels=model_channels_decoder,
            latent_channels=embed_dim,
            num_blocks=num_blocks_decoder,
            num_heads=num_heads_decoder,
            num_head_channels=num_head_channels_decoder,
            mlp_ratio=mlp_ratio,
            attn_mode=attn_mode,
            window_size=window_size,
            pe_mode=pe_mode,
            use_fp16=use_fp16,
            use_checkpoint=use_checkpoint,
            qk_rms_norm=qk_rms_norm,
            representation_config=representation_config,
            out_channels=out_channels,
            chunk_size=chunk_size,
        )
        
        # Vector Quantizerï¼ˆæ›¿ä»£ VAE çš„é«˜æ–¯åˆ†å¸ƒï¼‰
        self.vq = SparseVectorQuantizer(
            num_embeddings=num_embeddings,
            embedding_dim=embed_dim,
            beta=0.25,
            use_ema_update=use_ema_update,
            decay=vq_decay,
            epsilon=vq_epsilon
        )
        
        self.embed_dim = embed_dim
        self.use_ema_update = use_ema_update

    def forward(self, batch):
        """
        è®­ç»ƒæ—¶çš„å®Œæ•´å‰å‘ä¼ æ’­
        """
        z, vq_loss, commitment_loss = self.encode(batch)

        print(f"[DEBUG forward] Calling decoder...")
        reconst_x = self.decoder(z)
        print(f"[DEBUG forward] Decoder output: shape={reconst_x.shape}, feats.shape={reconst_x.feats.shape}")
        print(f"[DEBUG forward] Decoder output feats: min={reconst_x.feats.min().item():.6f}, max={reconst_x.feats.max().item():.6f}, mean={reconst_x.feats.mean().item():.6f}")
        print(f"[DEBUG forward] Decoder output requires_grad: {reconst_x.feats.requires_grad}")
        
        outputs = {
            'reconst_x': reconst_x, 
            'vq_loss': vq_loss,
            'commitment_loss': commitment_loss
        }
        return outputs

    def encode(self, batch, only_return_indices: bool = False):
        """
        ç¼–ç è¿‡ç¨‹ï¼Œæ›¿ä»£ VAE çš„é‡‡æ ·è¿‡ç¨‹
        Args:
            batch: è¾“å…¥æ•°æ®æ‰¹æ¬¡ã€‚å¯ä»¥æ˜¯ï¼š
                  - SparseTensorï¼šè®­ç»ƒæ—¶ä½¿ç”¨
                  - dictï¼šæ¨ç†æ—¶ä½¿ç”¨ï¼ŒåŒ…å« 'sparse_sdf', 'sparse_index', 'batch_idx' é”®
            only_return_indices: æ˜¯å¦åªè¿”å›é‡åŒ–ç´¢å¼•ï¼ˆç”¨äºæ¨ç†ï¼‰
        Returns:
            å¦‚æœ only_return_indices=True: è¿”å› encoding_indices
            å¦åˆ™: è¿”å› (z, vq_loss, commitment_loss)
        """
        # åˆ¤æ–­ batch çš„ç±»å‹å¹¶å¤„ç†
        if hasattr(batch, 'feats') and hasattr(batch, 'coords'):
            # batch æ˜¯ SparseTensorï¼ˆè®­ç»ƒæ—¶çš„æƒ…å†µï¼‰
            x = batch
            factor = None
        elif isinstance(batch, dict):
            # batch æ˜¯å­—å…¸ï¼ˆæ¨ç†æ—¶çš„æƒ…å†µï¼‰
            feat, xyz, batch_idx = batch['sparse_sdf'], batch['sparse_index'], batch['batch_idx']
            
            if feat.ndim == 1:
                feat = feat.unsqueeze(-1)
            
            coords = torch.cat([batch_idx.unsqueeze(-1), xyz], dim=-1).int()
            x = sp.SparseTensor(feat, coords)
            factor = batch.get('factor', None)
        else:
            raise TypeError(f"batch must be either SparseTensor or dict, got {type(batch)}")
        
        print(f"[DEBUG encode] Input x.feats: shape={x.feats.shape}, min={x.feats.min().item():.6f}, max={x.feats.max().item():.6f}, mean={x.feats.mean().item():.6f}, std={x.feats.std().item():.6f}")
        print(f"[DEBUG encode] Encoder training: {self.encoder.training}")
        
        h = self.encoder(x, factor)
        print(f"[DEBUG encode] Encoder output h.feats: shape={h.feats.shape}, min={h.feats.min().item():.6f}, max={h.feats.max().item():.6f}, mean={h.feats.mean().item():.6f}, std={h.feats.std().item():.6f}")
        print(f"[DEBUG encode] h.feats requires_grad: {h.feats.requires_grad}")
        
        if only_return_indices:
            # åªè¿”å›é‡åŒ–ç´¢å¼•ï¼ˆç”¨äº Encode æ–¹æ³•ï¼‰
            encoding_indices = self.vq(h, only_return_indices=True)
            return encoding_indices
        
        # é‡åŒ–ï¼ˆæ›¿ä»£ VAE çš„é‡‡æ ·ï¼‰
        quantized, vq_loss, commitment_loss, _ = self.vq(h)
        if vq_loss is not None:
            print(f"[DEBUG encode] Quantization results: vq_loss={vq_loss.item():.6f}, commitment_loss={commitment_loss.item():.6f}")
        else:
            print(f"[DEBUG encode] Quantization results: vq_loss=None (EMA mode), commitment_loss={commitment_loss.item():.6f}")

        return quantized, vq_loss, commitment_loss
    
    def Encode(self, batch):
        """
        ç¼–ç åˆ°ç¦»æ•£ç´¢å¼•ï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼‰
        Args:
            batch: è¾“å…¥æ•°æ®æ‰¹æ¬¡
        Returns:
            encoding_indices: SparseTensorï¼ŒåŒ…å«é‡åŒ–åçš„ indices
        """
        encoding_indices = self.encode(batch, only_return_indices=True)
        return encoding_indices
    
    def Decode(self, encoding_indices: sp.SparseTensor):
        """
        ä»ç¦»æ•£ç´¢å¼•è§£ç ï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼‰
        Args:
            encoding_indices: SparseTensorï¼ŒåŒ…å«é‡åŒ– indices
        Returns:
            recon: é‡å»ºçš„ SparseTensor
        """
        # ä» indices è·å– embedding
        indices = encoding_indices.feats.long().squeeze(-1)  # [N]
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
        if indices.max() >= self.vq.embeddings.num_embeddings:
            print(f"[ERROR Decode] Index out of range! max index: {indices.max().item()}, codebook size: {self.vq.embeddings.num_embeddings}")
        
        quantized_feats = self.vq.embeddings(indices)  # [N, latent_channels]
        
        # åˆ›å»º quantized SparseTensor
        quantized = encoding_indices.replace(quantized_feats)
        
        # è§£ç 
        recon = self.decoder(quantized)
        return recon

    def decode_mesh(self,
                    latents,
                    voxel_resolution: int = 512,
                    mc_threshold: float = 0.2,
                    return_feat: bool = False,
                    factor: float = 1.0):
        voxel_resolution = int(voxel_resolution / factor)
        reconst_x = self.decoder(latents, factor=factor, return_feat=return_feat)
        if return_feat:
            return reconst_x
        outputs = self.sparse2mesh(reconst_x, voxel_resolution=voxel_resolution, mc_threshold=mc_threshold)
        
        return outputs

    def sparse2mesh(self,
                    reconst_x: torch.FloatTensor,
                    voxel_resolution: int = 512,
                    mc_threshold: float = 0.0):

        sparse_sdf, sparse_index = reconst_x.feats.float(), reconst_x.coords
        batch_size = int(sparse_index[..., 0].max().cpu().numpy() + 1)

        meshes = []
        for i in range(batch_size):
            idx = sparse_index[..., 0] == i
            sparse_sdf_i, sparse_index_i = sparse_sdf[idx].squeeze(-1).cpu(),  sparse_index[idx][..., 1:].detach().cpu()
            sdf = torch.ones((voxel_resolution, voxel_resolution, voxel_resolution))
            sdf[sparse_index_i[..., 0], sparse_index_i[..., 1], sparse_index_i[..., 2]] = sparse_sdf_i
            vertices, faces, _, _ = measure.marching_cubes(
                sdf.numpy(),
                mc_threshold,
                method="lewiner",
            )
            vertices = vertices / voxel_resolution * 2 - 1
            meshes.append(trimesh.Trimesh(vertices, faces))

        return meshes
    
    @torch.no_grad()
    def load_pretrained_vae(self, encoder_state_dict: dict, decoder_state_dict: dict, vq_state_dict: dict = None):
        """
        åŠ è½½é¢„è®­ç»ƒçš„ VAE å‚æ•°
        Args:
            encoder_state_dict: é¢„è®­ç»ƒçš„ encoder æƒé‡å­—å…¸
            decoder_state_dict: é¢„è®­ç»ƒçš„ decoder æƒé‡å­—å…¸
            vq_state_dict: é¢„è®­ç»ƒçš„ VQ æƒé‡å­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        # åŠ è½½ encoder å‚æ•°
        encoder_dict = self.encoder.state_dict()
        encoder_dict.update(encoder_state_dict)
        self.encoder.load_state_dict(encoder_dict, strict=False)
        
        # åŠ è½½ decoder å‚æ•°
        decoder_dict = self.decoder.state_dict()
        decoder_dict.update(decoder_state_dict)
        self.decoder.load_state_dict(decoder_dict, strict=False)
        
        print(f"âœ… Loaded pretrained VAE parameters")
        print(f"   Encoder: {len(encoder_state_dict)} parameters loaded")
        print(f"   Decoder: {len(decoder_state_dict)} parameters loaded")
        
        # åŠ è½½ VQ å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        if vq_state_dict is not None and len(vq_state_dict) > 0:
            vq_dict = self.vq.state_dict()
            
            # ç­›é€‰å¯ç”¨çš„å‚æ•°ï¼ˆé¿å…å½¢çŠ¶ä¸åŒ¹é…ï¼‰
            loaded_keys = []
            skipped_keys = []
            for key, value in vq_state_dict.items():
                if key in vq_dict:
                    if vq_dict[key].shape == value.shape:
                        vq_dict[key] = value
                        loaded_keys.append(key)
                    else:
                        skipped_keys.append(f"{key} (shape mismatch: {vq_dict[key].shape} vs {value.shape})")
                else:
                    skipped_keys.append(f"{key} (not found in current model)")
            
            # åŠ è½½æ›´æ–°åçš„å‚æ•°
            self.vq.load_state_dict(vq_dict, strict=False)
            
            print(f"   VQ: {len(loaded_keys)} parameters loaded")
            if loaded_keys:
                print(f"      Loaded: {', '.join(loaded_keys)}")
            if skipped_keys:
                print(f"      Skipped: {', '.join(skipped_keys)}")
            
            # ç‰¹åˆ«è¯´æ˜EMA bufferçš„å¤„ç†
            if self.use_ema_update:
                if 'ema_cluster_size' in loaded_keys and 'ema_w' in loaded_keys:
                    print(f"      â„¹ï¸  EMA buffers loaded from pretrained model")
                else:
                    print(f"      âš ï¸  EMA buffers not found in pretrained model, will be initialized from scratch")
        else:
            print(f"   VQ: No pretrained VQ parameters provided, using random initialization")
    
    # def convert_to_fp16(self) -> None:
    #     """
    #     Convert encoder, decoder, and VQ codebook to float16.
    #     This method is called by the trainer when loading checkpoints with fp16_mode='inflat_all'.
    #     """
    #     if hasattr(self.encoder, 'convert_to_fp16'):
    #         self.encoder.convert_to_fp16()
    #     if hasattr(self.decoder, 'convert_to_fp16'):
    #         self.decoder.convert_to_fp16()
    #     # Convert VQ codebook embeddings to fp16
    #     if hasattr(self.vq, 'embeddings'):
    #         self.vq.embeddings.weight.data = self.vq.embeddings.weight.data.half()
    
    # def convert_to_fp32(self) -> None:
    #     """
    #     Convert encoder, decoder, and VQ codebook to float32.
    #     """
    #     if hasattr(self.encoder, 'convert_to_fp32'):
    #         self.encoder.convert_to_fp32()
    #     if hasattr(self.decoder, 'convert_to_fp32'):
    #         self.decoder.convert_to_fp32()
    #     # Convert VQ codebook embeddings to fp32
    #     if hasattr(self.vq, 'embeddings'):
    #         self.vq.embeddings.weight.data = self.vq.embeddings.weight.data.float()


# å‘åå…¼å®¹çš„åˆ«å
Direct3DS2_VQVAE = SparseSDFVQVAE
