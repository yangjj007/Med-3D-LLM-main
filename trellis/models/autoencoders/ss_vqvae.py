# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist

import trimesh
from skimage import measure
from sklearn import cluster

from ...modules import sparse as sp
from .encoder import SparseSDFEncoder
from .decoder import SparseSDFDecoder
# VQVAEä¸éœ€è¦DiagonalGaussianDistributionï¼ˆç§»é™¤VAEçš„é«˜æ–¯é‡‡æ ·æœºåˆ¶ï¼‰


class ReservoirSampler(nn.Module):
    """
    æ°´å¡˜é‡‡æ ·å™¨ï¼Œç”¨äºæŒç»­æ”¶é›†è®­ç»ƒç‰¹å¾
    ä½¿ç”¨ç»å…¸æ°´å¡˜é‡‡æ ·ç®—æ³•ï¼Œç»´æŠ¤å›ºå®šå¤§å°çš„ç‰¹å¾æ± 
    """
    def __init__(self, num_samples: int = 16384):
        super(ReservoirSampler, self).__init__()
        self.n = num_samples  # å®¹é‡
        self.register_buffer('buffer', None, persistent=False)
        self.register_buffer('i', torch.tensor(0), persistent=False)
        
    def add(self, samples: torch.Tensor):
        """
        æ·»åŠ æ ·æœ¬åˆ°æ°´å¡˜ï¼ˆæ¯ä¸ª GPU ç‹¬ç«‹ç»´æŠ¤ï¼‰
        Args:
            samples: [N, D] ç‰¹å¾å¼ é‡
        """
        if samples.numel() == 0:
            return
            
        samples = samples.detach()
        
        # åˆå§‹åŒ– buffer
        if self.buffer is None:
            self.buffer = torch.empty(self.n, samples.size(-1), 
                                     device=samples.device, dtype=samples.dtype)
            self.i = torch.tensor(0, device=samples.device)
        
        # ç¡®ä¿ buffer å’Œ samples åœ¨åŒä¸€è®¾å¤‡ä¸Š
        if self.buffer.device != samples.device:
            self.buffer = self.buffer.to(samples.device)
            self.i = self.i.to(samples.device)
        
        # æ¯ä¸ª GPU ç‹¬ç«‹è¿›è¡Œæ°´å¡˜é‡‡æ ·ï¼ˆä¸è¿›è¡Œè·¨ GPU åŒæ­¥ï¼‰
        for sample in samples:
            if self.i < self.n:
                # ç¼“å†²åŒºæœªæ»¡ï¼Œç›´æ¥æ·»åŠ 
                self.buffer[self.i] = sample
                self.i += 1
            else:
                # ç¼“å†²åŒºå·²æ»¡ï¼Œéšæœºæ›¿æ¢ï¼ˆæ°´å¡˜é‡‡æ ·ç®—æ³•ï¼‰
                j = torch.randint(0, self.i + 1, (1,), device=sample.device).item()
                if j < self.n:
                    self.buffer[j] = sample
                self.i += 1
    
    def contents(self) -> torch.Tensor:
        """
        è·å–é‡‡æ ·ç»“æœ
        Returns:
            [min(i, n), D] å·²æ”¶é›†çš„ç‰¹å¾
        """
        if self.buffer is None:
            return torch.empty(0)
        return self.buffer[:min(self.i.item(), self.n)]
    
    def reset(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        if self.buffer is not None:
            self.i.fill_(0)


class SparseVectorQuantizer(nn.Module):
    """
    ç¨€ç–å¼ é‡çš„ Vector Quantizer
    æ”¯æŒä¸¤ç§ç æœ¬æ›´æ–°æ¨¡å¼ï¼š
    1. æ¢¯åº¦æ›´æ–°æ¨¡å¼ (use_ema_update=False): é€šè¿‡åå‘ä¼ æ’­æ›´æ–°ç æœ¬
    2. EMAæ›´æ–°æ¨¡å¼ (use_ema_update=True): é€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡ç»Ÿè®¡æ›´æ–°ç æœ¬
    æ”¯æŒå¯é€‰çš„ K-means å‘¨æœŸæ€§é‡æ–°åˆå§‹åŒ–ï¼š
    3. K-means é‡ä¼°è®¡æ¨¡å¼ (use_kmeans_reinit=True): å‘¨æœŸæ€§ä½¿ç”¨ K-means èšç±»é‡ç½®ç æœ¬
    """
    def __init__(self, num_embeddings: int = 8192, embedding_dim: int = 64, beta: float = 0.25,
                 use_ema_update: bool = False, decay: float = 0.99, epsilon: float = 1e-5,
                 use_kmeans_reinit: bool = False, kmeans_interval: int = 2000, 
                 reservoir_size: int = 16384):
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.beta = beta
        self.use_ema_update = use_ema_update
        self.decay = decay
        self.epsilon = epsilon
        
        # K-means é‡ä¼°è®¡å‚æ•°
        self.use_kmeans_reinit = use_kmeans_reinit
        self.kmeans_interval = kmeans_interval
        
        # ç æœ¬åµŒå…¥
        self.embeddings = nn.Embedding(self.num_embeddings, self.embedding_dim)
        self.embeddings.weight.data.normal_(mean=0.0, std=1.0)
        
        # æ ¹æ®æ›´æ–°æ¨¡å¼è®¾ç½®requires_gradå’Œåˆå§‹åŒ–buffer
        if use_ema_update:
            # EMAæ¨¡å¼ï¼šç¦ç”¨æ¢¯åº¦ï¼Œæ³¨å†Œç»Ÿè®¡buffer
            self.embeddings.weight.requires_grad = False
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨0åˆå§‹åŒ–ï¼Œåœ¨ç¬¬ä¸€ä¸ªbatchåæ‰å¼€å§‹EMAæ›´æ–°
            # è¿™é¿å…äº†åˆå§‹åŒ–å¸¦æ¥çš„å‡è®¾ï¼Œè®©ç æœ¬å®Œå…¨ç”±æ•°æ®é©±åŠ¨
            # å¯¹äºæœªä½¿ç”¨çš„ç æœ¬ï¼Œé€šè¿‡æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ä¿æŒä¸ºåˆå§‹å€¼
            self.register_buffer('ema_cluster_size', torch.zeros(num_embeddings))
            self.register_buffer('ema_w', torch.zeros(num_embeddings, embedding_dim))
            # æ ‡è®°æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡EMAæ›´æ–°
            self.register_buffer('_ema_initialized', torch.tensor(False))
        # else: æ¢¯åº¦æ¨¡å¼ä¿æŒé»˜è®¤requires_grad=True
        
        # K-means é‡ä¼°è®¡å™¨
        if use_kmeans_reinit:
            self.reestimation_reservoir = ReservoirSampler(reservoir_size)
            print(f"[K-means é‡ä¼°è®¡] å·²å¯ç”¨ï¼Œé—´éš”={kmeans_interval}æ­¥ï¼Œæ°´å¡˜å®¹é‡={reservoir_size}")
    

    def forward(self, z: sp.SparseTensor, only_return_indices: bool = False, current_step: int = -1):
        """
        Args:
            z: SparseTensorï¼Œfeats shape ä¸º [N, embedding_dim]ï¼ŒN æ˜¯æ¿€æ´»ä½“ç´ æ•°é‡
            only_return_indices: æ˜¯å¦åªè¿”å› indices
            current_step: å½“å‰è®­ç»ƒæ­¥æ•°ï¼Œç”¨äº K-means é‡ä¼°è®¡è§¦å‘ï¼ˆ-1 è¡¨ç¤ºä¸ä½¿ç”¨ï¼‰
        Returns:
            å¦‚æœ only_return_indices=True: è¿”å› indices çš„ SparseTensor
            å¦åˆ™: è¿”å› (quantized, vq_loss, commitment_loss, encoding_indices, codebook_stats)
            æ³¨æ„ï¼šå½“use_ema_update=Trueæ—¶ï¼Œvq_lossä¸ºNone
            codebook_stats: åŒ…å« perplexity, entropy, unique_count, utilization_ratio çš„å­—å…¸
        """
        print(f"\n[DEBUG VQ] Input z.feats: shape={z.feats.shape}, min={z.feats.min().item():.6f}, max={z.feats.max().item():.6f}, mean={z.feats.mean().item():.6f}, std={z.feats.std().item():.6f}")
        print(f"[DEBUG VQ] Codebook: min={self.embeddings.weight.min().item():.6f}, max={self.embeddings.weight.max().item():.6f}, mean={self.embeddings.weight.mean().item():.6f}, std={self.embeddings.weight.std().item():.6f}")
        print(f"[DEBUG VQ] Codebook requires_grad: {self.embeddings.weight.requires_grad}, use_ema_update: {self.use_ema_update}")
        
        # æ£€æŸ¥ç æœ¬æ˜¯å¦æœ‰å¼‚å¸¸ï¼ˆå¤ªå¤šé›¶å‘é‡ï¼‰
        codebook_norms = torch.norm(self.embeddings.weight, dim=1)  # [num_embeddings]
        zero_codes = (codebook_norms < 0.01).sum().item()
        print(f"[DEBUG VQ] Codebook norms: min={codebook_norms.min().item():.6f}, max={codebook_norms.max().item():.6f}, mean={codebook_norms.mean().item():.6f}")
        print(f"[DEBUG VQ] Near-zero codes (norm<0.01): {zero_codes}/{self.num_embeddings}")
        
        # z.feats: [N, embedding_dim]
        z_flatten_orig = z.feats  # [N, embedding_dim]ï¼Œä¿ç•™åŸå§‹ dtype ç”¨äº loss è®¡ç®—
        input_dtype = z_flatten_orig.dtype
        emb_dtype = self.embeddings.weight.dtype
        
        # å°† z_flatten è½¬æ¢ä¸ºä¸ embeddings ç›¸åŒçš„ dtypeï¼Œé¿å… cdist dtype ä¸åŒ¹é…
        z_flatten = z_flatten_orig.to(emb_dtype)
        
        # è®¡ç®—è·ç¦»å¹¶æ‰¾åˆ°æœ€è¿‘çš„ codebook entry
        distances = torch.cdist(z_flatten, self.embeddings.weight)  # [N, num_embeddings]
        print(f"[DEBUG VQ] Distances: min={distances.min().item():.6f}, max={distances.max().item():.6f}, mean={distances.mean().item():.6f}")
        
        # ç»Ÿè®¡æœ€å°è·ç¦»çš„åˆ†å¸ƒ
        min_distances = distances.min(dim=1)[0]  # [N]
        print(f"[DEBUG VQ] Min distances: mean={min_distances.mean().item():.6f}, std={min_distances.std().item():.6f}, median={min_distances.median().item():.6f}")
        
        encoding_indices = torch.argmin(distances, dim=1)  # [N]
        unique_codes_batch = torch.unique(encoding_indices)
        print(f"[DEBUG VQ] Encoding indices: batch unique codes={len(unique_codes_batch)}/{self.num_embeddings}")
        
        # ç»Ÿè®¡æ¯ä¸ªç æœ¬è¢«ä½¿ç”¨çš„æ¬¡æ•°
        if len(unique_codes_batch) < 100:
            counts = torch.bincount(encoding_indices, minlength=self.num_embeddings)
            used_counts = counts[counts > 0]
            print(f"[DEBUG VQ] Usage distribution: min={used_counts.min().item()}, max={used_counts.max().item()}, mean={used_counts.float().mean().item():.1f}")
        
        # ============ ç æœ¬åˆ©ç”¨ç‡ç»Ÿè®¡ï¼ˆæŒ‰æ¯ä¸ªæ ·æœ¬å•ç‹¬è®¡ç®—åå–å‡å€¼ï¼‰============
        batch_ids = z.coords[:, 0]  # [N] æ¯ä¸ªä½“ç´ æ‰€å±çš„æ ·æœ¬ç´¢å¼•
        unique_batch_ids = torch.unique(batch_ids)
        num_samples = len(unique_batch_ids)
        
        sample_perplexities = []
        sample_entropies = []
        sample_unique_counts = []
        epsilon = 1e-10
        
        for bid in unique_batch_ids:
            mask = batch_ids == bid
            sample_indices = encoding_indices[mask]  # å½“å‰æ ·æœ¬çš„ç æœ¬ç´¢å¼•
            
            sample_unique = torch.unique(sample_indices)
            sample_unique_counts.append(len(sample_unique))
            
            sample_onehot = F.one_hot(sample_indices, self.num_embeddings).float()
            sample_probs = torch.mean(sample_onehot, dim=0)
            sample_entropy = -torch.sum(sample_probs * torch.log(sample_probs + epsilon))
            sample_entropies.append(sample_entropy.item())
            sample_perplexities.append(torch.exp(sample_entropy).item())
        
        avg_perplexity = sum(sample_perplexities) / num_samples
        avg_entropy = sum(sample_entropies) / num_samples
        avg_unique_count = sum(sample_unique_counts) / num_samples
        avg_utilization_ratio = (avg_unique_count / self.num_embeddings) * 100.0
        
        print(f"[DEBUG VQ] Per-sample stats (mean of {num_samples} samples): "
              f"unique={avg_unique_count:.1f}, perplexity={avg_perplexity:.2f}, entropy={avg_entropy:.4f}")
        
        codebook_stats = {
            'perplexity': avg_perplexity,
            'entropy': avg_entropy,
            'unique_count': avg_unique_count,
            'utilization_ratio': avg_utilization_ratio,
            'batch_unique_count': len(unique_codes_batch),
        }
        
        if only_return_indices:
            # è¿”å› indices ä½œä¸º SparseTensorï¼Œä¿æŒåŸå§‹åæ ‡
            result = z.replace(encoding_indices.unsqueeze(-1).float())
            return result
        
        # é‡åŒ–ï¼ˆembeddings è¿”å› emb_dtypeï¼‰
        quantized_feats = self.embeddings(encoding_indices)  # [N, embedding_dim]ï¼Œdtype=emb_dtype
        # è½¬å›åŸå§‹ dtypeï¼Œä¿è¯åç»­ loss å’Œ straight-through ä¸ z_flatten_orig ä¸€è‡´
        quantized_feats = quantized_feats.to(input_dtype)
        print(f"[DEBUG VQ] Quantized feats: min={quantized_feats.min().item():.6f}, max={quantized_feats.max().item():.6f}, mean={quantized_feats.mean().item():.6f}")
        
        # è®¡ç®—commitment lossï¼ˆä¸¤ç§æ¨¡å¼éƒ½éœ€è¦ï¼Œå‡ä½¿ç”¨åŸå§‹ dtypeï¼‰
        commitment_loss = F.mse_loss(z_flatten_orig, quantized_feats.detach())
        
        # æ ¹æ®æ›´æ–°æ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
        if self.use_ema_update:
            # EMAæ¨¡å¼ï¼šåœ¨è®­ç»ƒæ—¶è°ƒç”¨EMAæ›´æ–°ï¼ˆä½¿ç”¨ emb_dtype çš„ z_flattenï¼‰
            if self.training:
                self._update_ema(encoding_indices, z_flatten)
            vq_loss = None  # EMAæ¨¡å¼ä¸éœ€è¦vq_loss
            print(f"[DEBUG VQ] EMA mode - Commitment Loss: {commitment_loss.item():.6f}, VQ Loss: None")
        else:
            # æ¢¯åº¦æ¨¡å¼ï¼šè®¡ç®—vq_lossç”¨äºåå‘ä¼ æ’­
            vq_loss = F.mse_loss(quantized_feats, z_flatten_orig.detach())
            print(f"[DEBUG VQ] Gradient mode - VQ Loss: {vq_loss.item():.6f}, Commitment Loss: {commitment_loss.item():.6f}")
            print(f"[DEBUG VQ] VQ Loss requires_grad: {vq_loss.requires_grad}, Commitment Loss requires_grad: {commitment_loss.requires_grad}")
        
        # Straight-through estimatorï¼ˆå‡åœ¨åŸå§‹ dtype ä¸‹æ‰§è¡Œï¼‰
        quantized_feats = z_flatten_orig + (quantized_feats - z_flatten_orig).detach()
        
        # åˆ›å»ºæ–°çš„ SparseTensor
        quantized = z.replace(quantized_feats)
        encoding_indices_st = z.replace(encoding_indices.unsqueeze(-1).float())
        
        print(f"[DEBUG VQ] Output quantized feats: min={quantized.feats.min().item():.6f}, max={quantized.feats.max().item():.6f}, requires_grad={quantized.feats.requires_grad}\n")
        
        # ============ K-means ç‰¹å¾æ”¶é›†å’Œå‘¨æœŸæ€§é‡ä¼°è®¡ ============
        if self.use_kmeans_reinit and self.training and current_step >= 0:
            # æ”¶é›†ç‰¹å¾åˆ°æ°´å¡˜é‡‡æ ·å™¨
            self.reestimation_reservoir.add(z_flatten)
            
            # å‘¨æœŸæ€§è§¦å‘ K-means é‡ä¼°è®¡
            if current_step > 0 and current_step % self.kmeans_interval == 0:
                self.reestimate()
        
        return quantized, vq_loss, commitment_loss, encoding_indices_st, codebook_stats
    
    @torch.no_grad()
    def reestimate(self):
        """
        ä½¿ç”¨ K-means é‡æ–°åˆå§‹åŒ–ç æœ¬
        å‚è€ƒ VQFR å®ç°ï¼Œç”¨ K-means èšç±»ä¸­å¿ƒæ•´ä½“æ›¿æ¢ç æœ¬æƒé‡
        åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ”¶é›†æ‰€æœ‰ GPU çš„æ ·æœ¬åè¿›è¡Œèšç±»
        """
        if not self.use_kmeans_reinit:
            return
        
        # è·å–å½“å‰ GPU æ”¶é›†çš„ç‰¹å¾
        encodings = self.reestimation_reservoir.contents()
        
        # å¦‚æœæ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼Œæ”¶é›†æ‰€æœ‰ GPU çš„æ ·æœ¬
        if dist.is_initialized():
            world_size = dist.get_world_size()
            rank = dist.get_rank()
            
            # è·å–æ‰€æœ‰ GPU çš„æ ·æœ¬æ•°é‡
            local_size = torch.tensor([encodings.shape[0]], device=encodings.device)
            size_list = [torch.zeros_like(local_size) for _ in range(world_size)]
            dist.all_gather(size_list, local_size)
            
            # è®¡ç®—æ€»æ ·æœ¬æ•°
            total_samples = sum(s.item() for s in size_list)
            
            if rank == 0:
                print(f'[K-means é‡ä¼°è®¡] æ”¶é›†åˆ°å„ GPU æ ·æœ¬æ•°: {[s.item() for s in size_list]}ï¼Œæ€»è®¡: {total_samples}')
            
            # æ£€æŸ¥æ€»æ ·æœ¬æ•°æ˜¯å¦è¶³å¤Ÿ
            if total_samples < self.num_embeddings:
                if rank == 0:
                    print(f'[K-means é‡ä¼°è®¡] è·³è¿‡ï¼šæ€»æ ·æœ¬æ•°ä¸è¶³ ({total_samples} < {self.num_embeddings})')
                # æ‰€æœ‰ GPU åŒæ­¥è·³è¿‡
                return
            
            # æ”¶é›†æ‰€æœ‰ GPU çš„æ ·æœ¬ï¼ˆåªåœ¨ rank 0 æ‰§è¡Œèšç±»ï¼‰
            if encodings.shape[0] > 0:
                # ç¡®ä¿å¼ é‡è¿ç»­
                encodings = encodings.contiguous()
            
            # ä½¿ç”¨ gather è€Œé all_gatherï¼ˆåªåœ¨ rank 0 æ”¶é›†ï¼‰
            if rank == 0:
                # å‡†å¤‡æ¥æ”¶ç¼“å†²åŒº
                max_size = max(s.item() for s in size_list)
                gathered_encodings = []
                
                for i in range(world_size):
                    if i == 0:
                        # rank 0 çš„æ ·æœ¬
                        if encodings.shape[0] > 0:
                            gathered_encodings.append(encodings)
                    else:
                        # ä»å…¶ä»– rank æ¥æ”¶
                        recv_size = size_list[i].item()
                        if recv_size > 0:
                            recv_tensor = torch.empty(recv_size, encodings.shape[1], 
                                                     device=encodings.device, dtype=encodings.dtype)
                            dist.recv(recv_tensor, src=i)
                            gathered_encodings.append(recv_tensor)
                
                # åˆå¹¶æ‰€æœ‰æ ·æœ¬
                all_encodings = torch.cat(gathered_encodings, dim=0) if gathered_encodings else encodings
            else:
                # å…¶ä»– rank å‘é€æ ·æœ¬åˆ° rank 0
                if encodings.shape[0] > 0:
                    dist.send(encodings, dst=0)
                all_encodings = None
            
            # åªåœ¨ rank 0 æ‰§è¡Œ K-means èšç±»
            if rank == 0:
                print(f'[K-means é‡ä¼°è®¡] å¼€å§‹ï¼Œä½¿ç”¨ {all_encodings.shape[0]} ä¸ªæ ·æœ¬é‡å»º {self.num_embeddings} ä¸ªç æœ¬å‘é‡...')
                
                try:
                    # è½¬æ¢ä¸º numpy è¿›è¡Œèšç±»
                    encodings_np = all_encodings.cpu().numpy()
                    
                    # ä½¿ç”¨ sklearn çš„ K-means è¿›è¡Œèšç±»
                    clustered, *_ = cluster.k_means(encodings_np, self.num_embeddings, random_state=0)
                    
                    # ç”¨ K-means çš„èšç±»ä¸­å¿ƒæ•´ä½“æ›¿æ¢ç æœ¬
                    new_embeddings = torch.tensor(clustered, 
                                                  dtype=self.embeddings.weight.dtype,
                                                  device=self.embeddings.weight.device)
                    self.embeddings.weight.data[...] = new_embeddings
                    
                    print(f'[K-means é‡ä¼°è®¡] å®Œæˆï¼ç æœ¬å·²æ›´æ–°')
                    
                except Exception as e:
                    print(f'[K-means é‡ä¼°è®¡] å¤±è´¥ï¼š{e}')
            
            # å¹¿æ’­æ›´æ–°åçš„ç æœ¬åˆ°æ‰€æœ‰ GPU
            dist.broadcast(self.embeddings.weight.data, src=0)
            
            # æ‰€æœ‰ GPU åŒæ­¥æ¸…ç©ºæ°´å¡˜é‡‡æ ·å™¨
            self.reestimation_reservoir.reset()
            
            # å¦‚æœæ˜¯ EMA æ¨¡å¼ï¼Œä¹Ÿé‡ç½® EMA ç»Ÿè®¡é‡
            if self.use_ema_update:
                self.ema_cluster_size.zero_()
                self.ema_w.zero_()
                self._ema_initialized.fill_(False)
                if rank == 0:
                    print(f'[K-means é‡ä¼°è®¡] åŒæ—¶é‡ç½®äº† EMA ç»Ÿè®¡é‡')
        
        else:
            # å• GPU æ¨¡å¼
            if encodings.shape[0] < self.num_embeddings:
                print(f'[K-means é‡ä¼°è®¡] è·³è¿‡ï¼šæ ·æœ¬æ•°ä¸è¶³ ({encodings.shape[0]} < {self.num_embeddings})')
                return
            
            print(f'[K-means é‡ä¼°è®¡] å¼€å§‹ï¼Œä½¿ç”¨ {encodings.shape[0]} ä¸ªæ ·æœ¬é‡å»º {self.num_embeddings} ä¸ªç æœ¬å‘é‡...')
            
            # è½¬æ¢ä¸º numpy è¿›è¡Œèšç±»
            encodings_np = encodings.cpu().numpy()
            
            try:
                # ä½¿ç”¨ sklearn çš„ K-means è¿›è¡Œèšç±»
                clustered, *_ = cluster.k_means(encodings_np, self.num_embeddings, random_state=0)
                
                # ç”¨ K-means çš„èšç±»ä¸­å¿ƒæ•´ä½“æ›¿æ¢ç æœ¬
                self.embeddings.weight.data[...] = torch.tensor(clustered, 
                                                                dtype=self.embeddings.weight.dtype,
                                                                device=self.embeddings.weight.device)
                
                # æ¸…ç©ºæ°´å¡˜é‡‡æ ·å™¨ï¼Œé‡æ–°æ”¶é›†ç‰¹å¾
                self.reestimation_reservoir.reset()
                
                # å¦‚æœæ˜¯ EMA æ¨¡å¼ï¼Œä¹Ÿé‡ç½® EMA ç»Ÿè®¡é‡
                if self.use_ema_update:
                    self.ema_cluster_size.zero_()
                    self.ema_w.zero_()
                    self._ema_initialized.fill_(False)
                    print(f'[K-means é‡ä¼°è®¡] åŒæ—¶é‡ç½®äº† EMA ç»Ÿè®¡é‡')
                
                print(f'[K-means é‡ä¼°è®¡] å®Œæˆï¼ç æœ¬å·²æ›´æ–°')
                
            except Exception as e:
                print(f'[K-means é‡ä¼°è®¡] å¤±è´¥ï¼š{e}')
    
    @torch.no_grad()
    def _update_ema(self, encoding_indices, z_flatten):
        """
        ä½¿ç”¨EMAæ›´æ–°ç æœ¬ï¼ˆä»…åœ¨use_ema_update=Trueæ—¶è°ƒç”¨ï¼‰
        
        Args:
            encoding_indices: åˆ†é…çš„ç æœ¬ç´¢å¼• [N]
            z_flatten: encoderè¾“å‡ºçš„ç‰¹å¾å‘é‡ [N, embedding_dim]
        """
        print(f"[DEBUG EMA] === Starting EMA Update ===")
        # EMA ç»Ÿè®¡å…¨ç¨‹åœ¨ float32 ä¸‹è¿›è¡Œï¼Œä¿è¯æ•°å€¼ç¨³å®šï¼›åªåœ¨å†™å› embeddings.weight æ—¶è½¬æ¢å›åŸ dtype
        z_flatten = z_flatten.float()
        print(f"[DEBUG EMA] Input z_flatten: shape={z_flatten.shape}, min={z_flatten.min().item():.6f}, max={z_flatten.max().item():.6f}, mean={z_flatten.mean().item():.6f}, std={z_flatten.std().item():.6f}")
        print(f"[DEBUG EMA] Encoding indices: shape={encoding_indices.shape}, unique codes={len(torch.unique(encoding_indices))}/{self.num_embeddings}")
        
        # è®¡ç®—one-hotç¼–ç 
        encodings = F.one_hot(encoding_indices, self.num_embeddings).float()  # [N, num_embeddings]
        print(f"[DEBUG EMA] One-hot encodings: shape={encodings.shape}, sum={encodings.sum().item():.1f}")
        
        # æ£€æŸ¥å½“å‰EMAçŠ¶æ€
        print(f"[DEBUG EMA] OLD ema_cluster_size: sum={self.ema_cluster_size.sum().item():.1f}, min={self.ema_cluster_size.min().item():.6f}, max={self.ema_cluster_size.max().item():.6f}")
        print(f"[DEBUG EMA] OLD ema_w: min={self.ema_w.min().item():.6f}, max={self.ema_w.max().item():.6f}, mean={self.ema_w.mean().item():.6f}")
        print(f"[DEBUG EMA] OLD embeddings: min={self.embeddings.weight.data.min().item():.6f}, max={self.embeddings.weight.data.max().item():.6f}, mean={self.embeddings.weight.data.mean().item():.6f}")
        
        # ç¬¬ä¸€æ¬¡åˆå§‹åŒ–ï¼šç›´æ¥ç”¨batchç»Ÿè®¡åˆå§‹åŒ–EMA
        if not self._ema_initialized:
            print(f"[DEBUG EMA] âš ï¸  First EMA update - Initializing from batch statistics")
            batch_cluster_size = encodings.sum(0)  # [num_embeddings]
            batch_w = encodings.t() @ z_flatten  # [num_embeddings, embedding_dim]
            
            # å¯¹äºæœªä½¿ç”¨çš„ç æœ¬ï¼Œä¿æŒåŸå§‹åˆå§‹åŒ–å€¼
            # å¯¹äºä½¿ç”¨è¿‡çš„ç æœ¬ï¼Œç”¨batchç»Ÿè®¡åˆå§‹åŒ–
            self.ema_cluster_size.copy_(batch_cluster_size)
            self.ema_w.copy_(batch_w)
            self._ema_initialized.fill_(True)
            
            print(f"[DEBUG EMA] Initialized ema_cluster_size: sum={self.ema_cluster_size.sum().item():.1f}, nonzero={(self.ema_cluster_size > 0).sum().item()}/{self.num_embeddings}")
            print(f"[DEBUG EMA] Initialized ema_w: min={self.ema_w.min().item():.6f}, max={self.ema_w.max().item():.6f}")
        
        # EMAæ›´æ–°ç»Ÿè®¡é‡
        batch_cluster_size = encodings.sum(0)  # [num_embeddings]
        print(f"[DEBUG EMA] Batch cluster size: sum={batch_cluster_size.sum().item():.1f}, nonzero={(batch_cluster_size > 0).sum().item()}/{self.num_embeddings}")
        
        new_cluster_size = self.decay * self.ema_cluster_size + (1 - self.decay) * batch_cluster_size
        print(f"[DEBUG EMA] NEW cluster_size: sum={new_cluster_size.sum().item():.1f}, min={new_cluster_size.min().item():.6f}, max={new_cluster_size.max().item():.6f}")
        
        # è®¡ç®—batchçš„åŠ æƒç‰¹å¾å’Œ
        batch_w = encodings.t() @ z_flatten  # [num_embeddings, embedding_dim]
        print(f"[DEBUG EMA] Batch_w (encodings.t() @ z_flatten): shape={batch_w.shape}, min={batch_w.min().item():.6f}, max={batch_w.max().item():.6f}, mean={batch_w.mean().item():.6f}")
        
        new_w = self.decay * self.ema_w + (1 - self.decay) * batch_w
        print(f"[DEBUG EMA] NEW ema_w: min={new_w.min().item():.6f}, max={new_w.max().item():.6f}, mean={new_w.mean().item():.6f}")
        
        # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼ˆé¿å…æŸäº›ç æœ¬ä»æœªè¢«ä½¿ç”¨ï¼‰
        n = new_cluster_size.sum()
        print(f"[DEBUG EMA] Total cluster size n: {n.item():.1f}")
        
        smoothed_cluster_size = (
            (new_cluster_size + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n
        )
        print(f"[DEBUG EMA] Smoothed cluster size: min={smoothed_cluster_size.min().item():.6f}, max={smoothed_cluster_size.max().item():.6f}, mean={smoothed_cluster_size.mean():.6f}")
        
        # æ›´æ–°ç æœ¬å‘é‡ï¼ˆåœ¨ float32 ä¸‹è®¡ç®—ï¼Œæœ€åè½¬å› embeddings åŸæœ‰ dtype å†å†™å›ï¼‰
        emb_dtype = self.embeddings.weight.data.dtype
        new_embeddings = torch.zeros(self.num_embeddings, self.embedding_dim,
                                     dtype=torch.float32, device=self.embeddings.weight.device)
        used_mask = new_cluster_size > 0
        new_embeddings[used_mask] = new_w[used_mask] / (smoothed_cluster_size[used_mask].unsqueeze(1) + 1e-7)
        new_embeddings[~used_mask] = self.embeddings.weight.data[~used_mask].float()  # ä¿æŒæœªä½¿ç”¨ç æœ¬ä¸å˜
        
        print(f"[DEBUG EMA] NEW embeddings (all codes): min={new_embeddings.min().item():.6f}, max={new_embeddings.max().item():.6f}, mean={new_embeddings.mean().item():.6f}, std={new_embeddings.std().item():.6f}")
        print(f"[DEBUG EMA] Used codes: {used_mask.sum().item()}/{self.num_embeddings}, Unused codes: {(~used_mask).sum().item()}/{self.num_embeddings}")
        
        # æ£€æŸ¥æ›´æ–°åçš„ç æœ¬ä¸­æ˜¯å¦æœ‰near-zeroå‘é‡
        updated_norms = torch.norm(new_embeddings[used_mask], dim=1)
        if len(updated_norms) > 0:
            print(f"[DEBUG EMA] Updated codes norms: min={updated_norms.min().item():.6f}, max={updated_norms.max().item():.6f}, mean={updated_norms.mean().item():.6f}")
        
        # å†™å› embeddings.weightï¼Œè½¬æ¢å›åŸå§‹ dtypeï¼ˆfp16 or fp32ï¼‰
        self.embeddings.weight.data.copy_(new_embeddings.to(emb_dtype))
        
        # æ›´æ–°bufferï¼ˆbuffer æœ¬èº«æ˜¯ float32ï¼Œç›´æ¥ copyï¼‰
        self.ema_cluster_size.copy_(new_cluster_size)
        self.ema_w.copy_(new_w)
        
        print(f"[DEBUG EMA] === EMA Update Complete ===\n")


class SparseSDFVQVAE(nn.Module):
    """
    Direct3D-S2 çš„ VQVAE ç‰ˆæœ¬
    ä¸¥æ ¼éµå¾ª SparseSDFVAE çš„ç»“æ„ï¼Œåªæ›¿æ¢ VQ éƒ¨åˆ†
    """
    def __init__(self, *,
                 embed_dim: int = None,
                 latent_channels: int = None,  # åˆ«åï¼Œå…¼å®¹æ—§é…ç½®
                 resolution: int = 64,
                 model_channels_encoder: int = None,
                 model_channels_decoder: int = None,
                 model_channels: int = None,  # åˆ«åï¼Œå…¼å®¹æ—§é…ç½®
                 num_blocks_encoder: int = None,
                 num_blocks_decoder: int = None,
                 num_blocks: int = None,  # åˆ«åï¼Œå…¼å®¹æ—§é…ç½®
                 num_heads_encoder: int = None,
                 num_heads_decoder: int = None,
                 num_heads: int = None,  # åˆ«åï¼Œå…¼å®¹æ—§é…ç½®
                 num_head_channels_encoder: int = 64,
                 num_head_channels_decoder: int = 64,
                 num_head_channels: int = None,  # åˆ«åï¼Œå…¼å®¹æ—§é…ç½®
                 out_channels: int = 1,
                 use_fp16: bool = False,
                 use_checkpoint: bool = False,
                 chunk_size: int = 1,
                 latents_scale: float = 1.0,
                 latents_shift: float = 0.0,
                 num_embeddings: int = 8192,
                 use_ema_update: bool = False,  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨EMAæ›´æ–°ç æœ¬
                 vq_decay: float = 0.99,        # æ–°å¢ï¼šEMAè¡°å‡ç‡
                 vq_epsilon: float = 1e-5,      # æ–°å¢ï¼šæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°
                 use_kmeans_reinit: bool = False,  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨K-meansé‡æ–°åˆå§‹åŒ–
                 kmeans_interval: int = 2000,   # æ–°å¢ï¼šK-meansé‡ä¼°è®¡é—´éš”
                 reservoir_size: int = 16384,   # æ–°å¢ï¼šæ°´å¡˜é‡‡æ ·å™¨å®¹é‡
                 mlp_ratio: float = 4,
                 attn_mode: str = "swin",
                 window_size: int = 8,
                 pe_mode: str = "ape",
                 qk_rms_norm: bool = False,
                 representation_config: dict = None):

        super().__init__()
        
        # å¤„ç†å‚æ•°åˆ«åï¼ˆå…¼å®¹æ—§é…ç½®æ–‡ä»¶ï¼‰
        if latent_channels is not None and embed_dim is None:
            embed_dim = latent_channels
        if embed_dim is None:
            embed_dim = 0
            
        if model_channels is not None:
            if model_channels_encoder is None:
                model_channels_encoder = model_channels
            if model_channels_decoder is None:
                model_channels_decoder = model_channels
        if model_channels_encoder is None:
            model_channels_encoder = 512
        if model_channels_decoder is None:
            model_channels_decoder = 512
            
        if num_blocks is not None:
            if num_blocks_encoder is None:
                num_blocks_encoder = num_blocks
            if num_blocks_decoder is None:
                num_blocks_decoder = num_blocks
        if num_blocks_encoder is None:
            num_blocks_encoder = 4
        if num_blocks_decoder is None:
            num_blocks_decoder = 4
            
        if num_heads is not None:
            if num_heads_encoder is None:
                num_heads_encoder = num_heads
            if num_heads_decoder is None:
                num_heads_decoder = num_heads
        if num_heads_encoder is None:
            num_heads_encoder = 8
        if num_heads_decoder is None:
            num_heads_decoder = 8
            
        if num_head_channels is not None:
            num_head_channels_encoder = num_head_channels
            num_head_channels_decoder = num_head_channels

        self.use_checkpoint = use_checkpoint
        self.resolution = resolution
        self.latents_scale = latents_scale
        self.latents_shift = latents_shift

        self.encoder = SparseSDFEncoder(
            resolution=resolution,
            in_channels=model_channels_encoder,
            model_channels=model_channels_encoder,
            latent_channels=embed_dim,
            num_blocks=num_blocks_encoder,
            num_heads=num_heads_encoder,
            num_head_channels=num_head_channels_encoder,
            mlp_ratio=mlp_ratio,
            attn_mode=attn_mode,
            window_size=window_size,
            pe_mode=pe_mode,
            use_fp16=use_fp16,
            use_checkpoint=use_checkpoint,
            qk_rms_norm=qk_rms_norm,
        )

        self.decoder = SparseSDFDecoder(
            resolution=resolution,
            model_channels=model_channels_decoder,
            latent_channels=embed_dim,
            num_blocks=num_blocks_decoder,
            num_heads=num_heads_decoder,
            num_head_channels=num_head_channels_decoder,
            mlp_ratio=mlp_ratio,
            attn_mode=attn_mode,
            window_size=window_size,
            pe_mode=pe_mode,
            use_fp16=use_fp16,
            use_checkpoint=use_checkpoint,
            qk_rms_norm=qk_rms_norm,
            representation_config=representation_config,
            out_channels=out_channels,
            chunk_size=chunk_size,
        )
        
        # Vector Quantizerï¼ˆæ›¿ä»£ VAE çš„é«˜æ–¯åˆ†å¸ƒï¼‰
        self.vq = SparseVectorQuantizer(
            num_embeddings=num_embeddings,
            embedding_dim=embed_dim,
            beta=0.25,
            use_ema_update=use_ema_update,
            decay=vq_decay,
            epsilon=vq_epsilon,
            use_kmeans_reinit=use_kmeans_reinit,
            kmeans_interval=kmeans_interval,
            reservoir_size=reservoir_size
        )
        
        self.embed_dim = embed_dim
        self.use_ema_update = use_ema_update
        self.use_kmeans_reinit = use_kmeans_reinit

    def forward(self, batch, current_step: int = -1):
        """
        è®­ç»ƒæ—¶çš„å®Œæ•´å‰å‘ä¼ æ’­
        Args:
            batch: è¾“å…¥æ•°æ®æ‰¹æ¬¡
            current_step: å½“å‰è®­ç»ƒæ­¥æ•°ï¼Œç”¨äº K-means é‡ä¼°è®¡ï¼ˆ-1 è¡¨ç¤ºä¸ä½¿ç”¨ï¼‰
        """
        z, vq_loss, commitment_loss, codebook_stats = self.encode(batch, current_step=current_step)

        print(f"[DEBUG forward] Calling decoder...")
        reconst_x = self.decoder(z)
        print(f"[DEBUG forward] Decoder output: shape={reconst_x.shape}, feats.shape={reconst_x.feats.shape}")
        print(f"[DEBUG forward] Decoder output feats: min={reconst_x.feats.min().item():.6f}, max={reconst_x.feats.max().item():.6f}, mean={reconst_x.feats.mean().item():.6f}")
        print(f"[DEBUG forward] Decoder output requires_grad: {reconst_x.feats.requires_grad}")
        
        outputs = {
            'reconst_x': reconst_x, 
            'vq_loss': vq_loss,
            'commitment_loss': commitment_loss,
            'codebook_stats': codebook_stats
        }
        return outputs

    def encode(self, batch, only_return_indices: bool = False, current_step: int = -1):
        """
        ç¼–ç è¿‡ç¨‹ï¼Œæ›¿ä»£ VAE çš„é‡‡æ ·è¿‡ç¨‹
        Args:
            batch: è¾“å…¥æ•°æ®æ‰¹æ¬¡ã€‚å¯ä»¥æ˜¯ï¼š
                  - SparseTensorï¼šè®­ç»ƒæ—¶ä½¿ç”¨
                  - dictï¼šæ¨ç†æ—¶ä½¿ç”¨ï¼ŒåŒ…å« 'sparse_sdf', 'sparse_index', 'batch_idx' é”®
            only_return_indices: æ˜¯å¦åªè¿”å›é‡åŒ–ç´¢å¼•ï¼ˆç”¨äºæ¨ç†ï¼‰
            current_step: å½“å‰è®­ç»ƒæ­¥æ•°ï¼Œç”¨äº K-means é‡ä¼°è®¡ï¼ˆ-1 è¡¨ç¤ºä¸ä½¿ç”¨ï¼‰
        Returns:
            å¦‚æœ only_return_indices=True: è¿”å› encoding_indices
            å¦åˆ™: è¿”å› (z, vq_loss, commitment_loss, codebook_stats)
        """
        # åˆ¤æ–­ batch çš„ç±»å‹å¹¶å¤„ç†
        if hasattr(batch, 'feats') and hasattr(batch, 'coords'):
            # batch æ˜¯ SparseTensorï¼ˆè®­ç»ƒæ—¶çš„æƒ…å†µï¼‰
            x = batch
            factor = None
        elif isinstance(batch, dict):
            # batch æ˜¯å­—å…¸ï¼ˆæ¨ç†æ—¶çš„æƒ…å†µï¼‰
            feat, xyz, batch_idx = batch['sparse_sdf'], batch['sparse_index'], batch['batch_idx']
            
            if feat.ndim == 1:
                feat = feat.unsqueeze(-1)
            
            coords = torch.cat([batch_idx.unsqueeze(-1), xyz], dim=-1).int()
            x = sp.SparseTensor(feat, coords)
            factor = batch.get('factor', None)
        else:
            raise TypeError(f"batch must be either SparseTensor or dict, got {type(batch)}")
        
        print(f"[DEBUG encode] Input x.feats: shape={x.feats.shape}, min={x.feats.min().item():.6f}, max={x.feats.max().item():.6f}, mean={x.feats.mean().item():.6f}, std={x.feats.std().item():.6f}")
        print(f"[DEBUG encode] Encoder training: {self.encoder.training}")
        
        h = self.encoder(x, factor)
        print(f"[DEBUG encode] Encoder output h.feats: shape={h.feats.shape}, min={h.feats.min().item():.6f}, max={h.feats.max().item():.6f}, mean={h.feats.mean().item():.6f}, std={h.feats.std().item():.6f}")
        print(f"[DEBUG encode] h.feats requires_grad: {h.feats.requires_grad}")
        
        if only_return_indices:
            # åªè¿”å›é‡åŒ–ç´¢å¼•ï¼ˆç”¨äº Encode æ–¹æ³•ï¼‰
            encoding_indices = self.vq(h, only_return_indices=True, current_step=current_step)
            return encoding_indices
        
        # é‡åŒ–ï¼ˆæ›¿ä»£ VAE çš„é‡‡æ ·ï¼‰
        quantized, vq_loss, commitment_loss, _, codebook_stats = self.vq(h, current_step=current_step)
        if vq_loss is not None:
            print(f"[DEBUG encode] Quantization results: vq_loss={vq_loss.item():.6f}, commitment_loss={commitment_loss.item():.6f}")
        else:
            print(f"[DEBUG encode] Quantization results: vq_loss=None (EMA mode), commitment_loss={commitment_loss.item():.6f}")

        return quantized, vq_loss, commitment_loss, codebook_stats
    
    def Encode(self, batch):
        """
        ç¼–ç åˆ°ç¦»æ•£ç´¢å¼•ï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼‰
        Args:
            batch: è¾“å…¥æ•°æ®æ‰¹æ¬¡
        Returns:
            encoding_indices: SparseTensorï¼ŒåŒ…å«é‡åŒ–åçš„ indices
        """
        encoding_indices = self.encode(batch, only_return_indices=True)
        return encoding_indices
    
    def Decode(self, encoding_indices: sp.SparseTensor):
        """
        ä»ç¦»æ•£ç´¢å¼•è§£ç ï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼‰
        Args:
            encoding_indices: SparseTensorï¼ŒåŒ…å«é‡åŒ– indices
        Returns:
            recon: é‡å»ºçš„ SparseTensor
        """
        # ä» indices è·å– embedding
        indices = encoding_indices.feats.long().squeeze(-1)  # [N]
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
        if indices.max() >= self.vq.embeddings.num_embeddings:
            print(f"[ERROR Decode] Index out of range! max index: {indices.max().item()}, codebook size: {self.vq.embeddings.num_embeddings}")
        
        quantized_feats = self.vq.embeddings(indices)  # [N, latent_channels]
        
        # åˆ›å»º quantized SparseTensor
        quantized = encoding_indices.replace(quantized_feats)
        
        # è§£ç 
        recon = self.decoder(quantized)
        return recon

    def decode_mesh(self,
                    latents,
                    voxel_resolution: int = 512,
                    mc_threshold: float = 0.2,
                    return_feat: bool = False,
                    factor: float = 1.0):
        voxel_resolution = int(voxel_resolution / factor)
        reconst_x = self.decoder(latents, factor=factor, return_feat=return_feat)
        if return_feat:
            return reconst_x
        outputs = self.sparse2mesh(reconst_x, voxel_resolution=voxel_resolution, mc_threshold=mc_threshold)
        
        return outputs

    def sparse2mesh(self,
                    reconst_x: torch.FloatTensor,
                    voxel_resolution: int = 512,
                    mc_threshold: float = 0.0):

        sparse_sdf, sparse_index = reconst_x.feats.float(), reconst_x.coords
        batch_size = int(sparse_index[..., 0].max().cpu().numpy() + 1)

        meshes = []
        for i in range(batch_size):
            idx = sparse_index[..., 0] == i
            sparse_sdf_i, sparse_index_i = sparse_sdf[idx].squeeze(-1).cpu(),  sparse_index[idx][..., 1:].detach().cpu()
            sdf = torch.ones((voxel_resolution, voxel_resolution, voxel_resolution))
            sdf[sparse_index_i[..., 0], sparse_index_i[..., 1], sparse_index_i[..., 2]] = sparse_sdf_i
            vertices, faces, _, _ = measure.marching_cubes(
                sdf.numpy(),
                mc_threshold,
                method="lewiner",
            )
            vertices = vertices / voxel_resolution * 2 - 1
            meshes.append(trimesh.Trimesh(vertices, faces))

        return meshes
    
    @torch.no_grad()
    def load_pretrained_vae(self, encoder_state_dict: dict, decoder_state_dict: dict, vq_state_dict: dict = None):
        """
        åŠ è½½é¢„è®­ç»ƒçš„ VAE å‚æ•°
        Args:
            encoder_state_dict: é¢„è®­ç»ƒçš„ encoder æƒé‡å­—å…¸
            decoder_state_dict: é¢„è®­ç»ƒçš„ decoder æƒé‡å­—å…¸
            vq_state_dict: é¢„è®­ç»ƒçš„ VQ æƒé‡å­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        print(f"\n{'='*80}")
        print(f"ğŸ”§ [DEBUG] SparseSDFVQVAE.load_pretrained_vae è¢«è°ƒç”¨")
        print(f"{'='*80}")
        print(f"ğŸ“Š è¾“å…¥å‚æ•°:")
        print(f"   encoder_state_dict: {len(encoder_state_dict) if encoder_state_dict else 0} ä¸ªå‚æ•°")
        print(f"   decoder_state_dict: {len(decoder_state_dict) if decoder_state_dict else 0} ä¸ªå‚æ•°")
        print(f"   vq_state_dict: {len(vq_state_dict) if vq_state_dict else 0} ä¸ªå‚æ•°")
        print(f"   vq_state_dict is None: {vq_state_dict is None}")
        print(f"   vq_state_dict is not None and len(vq_state_dict) > 0: {vq_state_dict is not None and len(vq_state_dict) > 0}")
        
        if vq_state_dict:
            print(f"\nğŸ“‹ VQ state_dict è¯¦æƒ…:")
            for key, value in vq_state_dict.items():
                if isinstance(value, torch.Tensor):
                    print(f"   - {key}: shape={value.shape}, dtype={value.dtype}")
        
        # åŠ è½½ encoder å‚æ•°
        print(f"\nğŸ“¥ åŠ è½½ Encoder å‚æ•°...")
        encoder_dict = self.encoder.state_dict()
        encoder_dict.update(encoder_state_dict)
        self.encoder.load_state_dict(encoder_dict, strict=False)
        print(f"   âœ… Encoder åŠ è½½å®Œæˆ")
        
        # åŠ è½½ decoder å‚æ•°
        print(f"\nğŸ“¥ åŠ è½½ Decoder å‚æ•°...")
        decoder_dict = self.decoder.state_dict()
        decoder_dict.update(decoder_state_dict)
        self.decoder.load_state_dict(decoder_dict, strict=False)
        print(f"   âœ… Decoder åŠ è½½å®Œæˆ")
        
        # å¼ºåˆ¶å°†encoderå’Œdecoderè½¬æ¢ä¸ºæ­£ç¡®çš„dtype
        # è¿™æ ·å¯ä»¥ç¡®ä¿å³ä½¿checkpointä¸­çš„æƒé‡æ˜¯float16ï¼Œä¹Ÿèƒ½æ­£ç¡®è½¬æ¢
        print(f"\nğŸ”§ æ£€æŸ¥å¹¶ä¿®æ­£ dtype...")
        if not self.encoder.use_fp16:
            print(f"   Encoder use_fp16=Falseï¼Œè½¬æ¢ä¸º float32")
            self.encoder.convert_to_fp32()
        else:
            print(f"   Encoder use_fp16=Trueï¼Œè½¬æ¢ä¸º float16")
            self.encoder.convert_to_fp16()
            
        if not self.decoder.use_fp16:
            print(f"   Decoder use_fp16=Falseï¼Œè½¬æ¢ä¸º float32")
            self.decoder.convert_to_fp32()
        else:
            print(f"   Decoder use_fp16=Trueï¼Œè½¬æ¢ä¸º float16")
            self.decoder.convert_to_fp16()
        
        print(f"\nâœ… Loaded pretrained VAE parameters")
        print(f"   Encoder: {len(encoder_state_dict)} parameters loaded")
        print(f"   Decoder: {len(decoder_state_dict)} parameters loaded")
        
        # åŠ è½½ VQ å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        if vq_state_dict is not None and len(vq_state_dict) > 0:
            print(f"\nğŸ“¥ åŠ è½½ VQ å‚æ•°...")
            print(f"   VQ state_dict åŒ…å« {len(vq_state_dict)} ä¸ªé”®")
            
            # è®°å½•åŠ è½½å‰çš„codebook
            print(f"\nğŸ“Š åŠ è½½å‰çš„ VQ Codebook:")
            before_embeddings = self.vq.embeddings.weight.data.clone()
            print(f"   Shape: {before_embeddings.shape}")
            print(f"   Min: {before_embeddings.min().item():.6f}, Max: {before_embeddings.max().item():.6f}")
            print(f"   Mean: {before_embeddings.mean().item():.6f}, Std: {before_embeddings.std().item():.6f}")
            print(f"   å‰3ä¸ªcodeçš„å‰5ç»´:")
            for i in range(min(3, before_embeddings.shape[0])):
                print(f"     Code {i}: {before_embeddings[i, :5].tolist()}")
            
            vq_dict = self.vq.state_dict()
            print(f"\nğŸ” å½“å‰ VQ æ¨¡å‹çš„ state_dict åŒ…å« {len(vq_dict)} ä¸ªé”®:")
            for key in vq_dict.keys():
                val = vq_dict[key]
                print(f"   - {key}: shape={val.shape if isinstance(val, torch.Tensor) else type(val)}")
            
            # ç­›é€‰å¯ç”¨çš„å‚æ•°ï¼ˆé¿å…å½¢çŠ¶ä¸åŒ¹é…ï¼‰
            loaded_keys = []
            skipped_keys = []
            print(f"\nğŸ”„ å¼€å§‹åŒ¹é…å’ŒåŠ è½½å‚æ•°...")
            for key, value in vq_state_dict.items():
                print(f"\n   æ£€æŸ¥é”®: {key}")
                if key in vq_dict:
                    print(f"     âœ“ é”®å­˜åœ¨äºæ¨¡å‹ä¸­")
                    print(f"     é¢„è®­ç»ƒ shape: {value.shape}")
                    print(f"     å½“å‰æ¨¡å‹ shape: {vq_dict[key].shape}")
                    if vq_dict[key].shape == value.shape:
                        print(f"     âœ“ Shape åŒ¹é…ï¼æ­£åœ¨æ›´æ–°...")
                        vq_dict[key] = value
                        loaded_keys.append(key)
                        print(f"     âœ… å·²æ›´æ–°åˆ° vq_dict")
                        
                        # å¦‚æœæ˜¯ embeddings.weightï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
                        if key == 'embeddings.weight':
                            print(f"     ğŸ“Š é¢„è®­ç»ƒ embeddings ç»Ÿè®¡:")
                            print(f"        Min: {value.min().item():.6f}, Max: {value.max().item():.6f}")
                            print(f"        Mean: {value.mean().item():.6f}, Std: {value.std().item():.6f}")
                            print(f"        å‰3ä¸ªcodeçš„å‰5ç»´:")
                            for i in range(min(3, value.shape[0])):
                                print(f"          Code {i}: {value[i, :5].tolist()}")
                    else:
                        print(f"     âœ— Shape ä¸åŒ¹é…ï¼Œè·³è¿‡")
                        skipped_keys.append(f"{key} (shape mismatch: {vq_dict[key].shape} vs {value.shape})")
                else:
                    print(f"     âœ— é”®ä¸å­˜åœ¨äºå½“å‰æ¨¡å‹")
                    skipped_keys.append(f"{key} (not found in current model)")
            
            # åŠ è½½æ›´æ–°åçš„å‚æ•°
            print(f"\nğŸ“¥ è°ƒç”¨ self.vq.load_state_dict()...")
            self.vq.load_state_dict(vq_dict, strict=False)
            print(f"   âœ… load_state_dict å®Œæˆ")
            
            # éªŒè¯åŠ è½½åçš„codebook
            print(f"\nğŸ“Š åŠ è½½åçš„ VQ Codebook:")
            after_embeddings = self.vq.embeddings.weight.data
            print(f"   Shape: {after_embeddings.shape}")
            print(f"   Min: {after_embeddings.min().item():.6f}, Max: {after_embeddings.max().item():.6f}")
            print(f"   Mean: {after_embeddings.mean().item():.6f}, Std: {after_embeddings.std().item():.6f}")
            print(f"   å‰3ä¸ªcodeçš„å‰5ç»´:")
            for i in range(min(3, after_embeddings.shape[0])):
                print(f"     Code {i}: {after_embeddings[i, :5].tolist()}")
            
            # è®¡ç®—å˜åŒ–
            diff = (after_embeddings - before_embeddings).abs().max().item()
            print(f"\n   ğŸ” åŠ è½½å‰åçš„æœ€å¤§å·®å¼‚: {diff:.6e}")
            if diff < 1e-6:
                print(f"   âš ï¸  è­¦å‘Š: Codebook å‡ ä¹æ²¡æœ‰å˜åŒ–ï¼å¯èƒ½åŠ è½½å¤±è´¥ï¼")
            else:
                print(f"   âœ… Codebook å·²æ›´æ–°ï¼")
            
            print(f"\n   VQ: {len(loaded_keys)} parameters loaded")
            if loaded_keys:
                print(f"      âœ… Loaded: {', '.join(loaded_keys)}")
            if skipped_keys:
                print(f"      âš ï¸  Skipped: {', '.join(skipped_keys)}")
            
            # ç‰¹åˆ«è¯´æ˜EMA bufferçš„å¤„ç†
            if self.use_ema_update:
                print(f"\n   ğŸ“‹ EMA æ¨¡å¼æ£€æŸ¥ (use_ema_update=True):")
                if 'ema_cluster_size' in loaded_keys and 'ema_w' in loaded_keys:
                    print(f"      âœ… EMA buffers loaded from pretrained model")
                else:
                    print(f"      âš ï¸  EMA buffers not found in pretrained model, will be initialized from scratch")
                    print(f"      å¯ç”¨çš„VQé”®: {list(vq_state_dict.keys())}")
        else:
            print(f"\n   âš ï¸  VQ: No pretrained VQ parameters provided or empty dict, using random initialization")
            if vq_state_dict is None:
                print(f"      åŸå› : vq_state_dict is None")
            elif len(vq_state_dict) == 0:
                print(f"      åŸå› : vq_state_dict is empty")
        
        print(f"{'='*80}\n")
    
    def convert_to_fp16(self) -> None:
        """
        Convert encoder, decoder, and VQ codebook to float16.
        This method is called by the trainer when loading checkpoints with fp16_mode='inflat_all'.
        """
        if hasattr(self.encoder, 'convert_to_fp16'):
            self.encoder.convert_to_fp16()
        if hasattr(self.decoder, 'convert_to_fp16'):
            self.decoder.convert_to_fp16()
        # Convert VQ codebook embeddings to fp16
        if hasattr(self.vq, 'embeddings'):
            self.vq.embeddings.weight.data = self.vq.embeddings.weight.data.half()
    
    def convert_to_fp32(self) -> None:
        """
        Convert encoder, decoder, and VQ codebook to float32.
        """
        if hasattr(self.encoder, 'convert_to_fp32'):
            self.encoder.convert_to_fp32()
        if hasattr(self.decoder, 'convert_to_fp32'):
            self.decoder.convert_to_fp32()
        # Convert VQ codebook embeddings to fp32
        if hasattr(self.vq, 'embeddings'):
            self.vq.embeddings.weight.data = self.vq.embeddings.weight.data.float()


# å‘åå…¼å®¹çš„åˆ«å
Direct3DS2_VQVAE = SparseSDFVQVAE
