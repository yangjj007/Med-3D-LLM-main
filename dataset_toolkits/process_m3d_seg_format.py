"""
M3D-Segæ ¼å¼æ•°æ®é¢„å¤„ç†è„šæœ¬

å¤„ç†å·²ç»æ˜¯NPYæ ¼å¼çš„M3D-Segæ•°æ®ï¼Œåº”ç”¨ç›¸åŒçš„é¢„å¤„ç†æµç¨‹ï¼š
- åˆ†è¾¨ç‡é€‚é…
- CTæ ‡å‡†åŒ–
- çª—å£äºŒå€¼åŒ–
- å™¨å®˜ç‰¹å®šçª—å£å¤„ç†

è¾“å…¥æ ¼å¼:
dataset_folder/
â”œâ”€â”€ 0000.json
â”œâ”€â”€ 1/
â”‚   â”œâ”€â”€ image.npy
â”‚   â””â”€â”€ mask_(1, 512, 512, 96).npz
â”œâ”€â”€ 2/
â”‚   â”œâ”€â”€ image.npy
â”‚   â””â”€â”€ mask_(...).npz
â””â”€â”€ ...
"""

import os
import sys
import json
import time
import argparse
import re
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Optional, List
from concurrent.futures import ProcessPoolExecutor, as_completed
from concurrent.futures.process import BrokenProcessPool
from tqdm import tqdm
import glob
from scipy import sparse
import ast
import multiprocessing

# è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•ä¸ºspawnï¼Œé¿å…CUDA forké—®é¢˜
# åœ¨å¤šè¿›ç¨‹ä¸­ä½¿ç”¨CUDAæ—¶å¿…é¡»ä½¿ç”¨spawnæ¨¡å¼
try:
    multiprocessing.set_start_method('spawn')
except RuntimeError:
    # å¦‚æœå·²ç»è®¾ç½®è¿‡ï¼Œå¿½ç•¥é”™è¯¯
    pass

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
from ct_preprocessing import (
    DEFAULT_RESOLUTION,
    determine_target_resolution,
    adapt_resolution,
    process_all_windows,
    process_all_organs,
    compute_organ_statistics,
    get_window_filename,
    validate_segmentation,
    get_organs_present
)


def normalize_ct(ct_array: np.ndarray) -> np.ndarray:
    """æ ‡å‡†åŒ–CTå›¾åƒï¼ˆä¸process_medical_ct.pyä¿æŒä¸€è‡´ï¼‰"""
    ct_voxel_ndarray = ct_array.flatten()
    
    thred = np.mean(ct_voxel_ndarray)
    voxel_filtered = ct_voxel_ndarray[ct_voxel_ndarray > thred]
    
    if len(voxel_filtered) == 0:
        voxel_filtered = ct_voxel_ndarray
    
    upper_bound = np.percentile(voxel_filtered, 99.95)
    lower_bound = np.percentile(voxel_filtered, 0.05)
    mean = np.mean(voxel_filtered)
    std = np.std(voxel_filtered)
    
    del ct_voxel_ndarray, voxel_filtered
    
    ct_normalized = np.clip(ct_array, lower_bound, upper_bound)
    ct_normalized = (ct_normalized - mean) / max(std, 1e-8)
    
    return ct_normalized


def load_m3d_seg_case(case_dir: str) -> tuple:
    """
    åŠ è½½M3D-Segæ ¼å¼çš„å•ä¸ªç—…ä¾‹
    
    Args:
        case_dir: ç—…ä¾‹ç›®å½•ï¼ˆå¦‚ dataset/1/ï¼‰
    
    Returns:
        (ct_array, seg_array, mask_shape)
    """
    # åŠ è½½image.npy
    image_path = os.path.join(case_dir, 'image.npy')
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Image not found: {image_path}")
    
    ct_array = np.load(image_path)
    
    # åŠ è½½maskæ–‡ä»¶
    mask_files = glob.glob(os.path.join(case_dir, 'mask_*.npz'))
    if not mask_files:
        return ct_array, None, None
    
    mask_file = mask_files[0]
    
    # ä»æ–‡ä»¶åè§£æå½¢çŠ¶
    mask_filename = os.path.basename(mask_file)
    # æå–æ‹¬å·ä¸­çš„å½¢çŠ¶ä¿¡æ¯
    # æ–‡ä»¶åæ ¼å¼: mask_(3, 512, 512, 633).npz
    shape_match = re.search(r'\([\d,\s]+\)', mask_filename)
    if not shape_match:
        print(f"  è­¦å‘Š: æ— æ³•ä»æ–‡ä»¶åè§£æmaskå½¢çŠ¶: {mask_filename}")
        return ct_array, None, None
    shape_str = shape_match.group(0)
    mask_shape = ast.literal_eval(shape_str)
    
    # åŠ è½½ç¨€ç–çŸ©é˜µ
    seg_sparse = sparse.load_npz(mask_file)
    seg_array = seg_sparse.toarray().reshape(mask_shape)
    
    return ct_array, seg_array, mask_shape


def load_dataset_json(json_path: str) -> Dict:
    """åŠ è½½æ•°æ®é›†JSONé…ç½®"""
    with open(json_path, 'r', encoding='utf-8') as f:
        dataset_info = json.load(f)
    return dataset_info


def build_organ_mapping_from_json(dataset_json: Dict, dataset_root: str = None) -> Dict:
    """
    ä»M3D-Segçš„JSONæ„å»ºå™¨å®˜æ˜ å°„
    
    M3D-Segæ•°æ®é›†è‡ªå¸¦æ ‡ç­¾ä¿¡æ¯ï¼Œç›´æ¥ä»æ•°æ®é›†JSONä¸­æå–ã€‚
    ä¸éœ€è¦ç”¨æˆ·é¢å¤–æä¾›organ_labels.jsonæ˜ å°„æ–‡ä»¶ã€‚
    
    æ ‡ç­¾ä¿¡æ¯æ¥æºï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
    1. æ•°æ®é›†JSONçš„'labels'å­—æ®µï¼ˆå­—å…¸ï¼šlabel_id -> label_nameï¼‰
    2. æ•°æ®é›†JSONçš„'label_names'å­—æ®µï¼ˆåˆ—è¡¨æˆ–å­—å…¸ï¼‰
    3. å¦‚æœdataset_rootæä¾›ï¼Œå°è¯•è¯»å–å…¨å±€dataset_info.json
    
    Args:
        dataset_json: M3D-Segæ•°æ®é›†JSONï¼ˆå¦‚0000.json, 0001.jsonç­‰ï¼‰
        dataset_root: æ•°æ®é›†æ ¹ç›®å½•ï¼ˆå¯é€‰ï¼Œç”¨äºæŸ¥æ‰¾dataset_info.jsonï¼‰
    
    Returns:
        å™¨å®˜æ˜ å°„é…ç½®å­—å…¸
    """
    organ_mapping = {
        'dataset_name': dataset_json.get('name', 'Unknown'),
        'modality': 'CT',
        'organ_labels': {}
    }
    
    # æ–¹æ³•1: ä»'labels'å­—æ®µè¯»å–ï¼ˆå­—å…¸æ ¼å¼ï¼š{label_id: label_name}ï¼‰
    labels = dataset_json.get('labels', {})
    print(f"  [è°ƒè¯•] ä»JSONè¯»å–çš„labels: {labels}")
    
    # æ–¹æ³•2: ä»'label_names'å­—æ®µè¯»å–ï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸ï¼‰
    if not labels:
        label_names = dataset_json.get('label_names', None)
        if isinstance(label_names, dict):
            labels = label_names
        elif isinstance(label_names, list):
            # åˆ—è¡¨æ ¼å¼ï¼šç´¢å¼•å°±æ˜¯label_id
            labels = {str(i): name for i, name in enumerate(label_names)}
    
    # æ–¹æ³•3: å°è¯•ä»å…¨å±€dataset_info.jsonè¯»å–
    if not labels and dataset_root:
        dataset_info_path = os.path.join(dataset_root, 'dataset_info.json')
        if os.path.exists(dataset_info_path):
            try:
                with open(dataset_info_path, 'r', encoding='utf-8') as f:
                    dataset_info = json.load(f)
                    # dataset_infoå¯èƒ½åŒ…å«å¤šä¸ªæ•°æ®é›†çš„ä¿¡æ¯
                    dataset_code = os.path.basename(dataset_root)
                    if dataset_code in dataset_info:
                        labels = dataset_info[dataset_code].get('labels', {})
                    print(f"  ä»dataset_info.jsonè¯»å–æ ‡ç­¾ä¿¡æ¯: {len(labels)}ä¸ªæ ‡ç­¾")
            except Exception as e:
                print(f"  è­¦å‘Š: æ— æ³•è¯»å–dataset_info.json: {e}")
    
    if not labels:
        print(f"  è­¦å‘Š: æœªæ‰¾åˆ°æ ‡ç­¾ä¿¡æ¯ï¼Œå°†åªå¤„ç†å…¨å±€çª—å£ï¼Œä¸å¤„ç†å™¨å®˜ç‰¹å®šçª—å£")
        return organ_mapping
    
    # è½¬æ¢æ ‡ç­¾æ ¼å¼å¹¶è‡ªåŠ¨æ¨æ–­çª—å£ç±»å‹
    for label_id, label_name in labels.items():
        print(f"  [è°ƒè¯•] å¤„ç†æ ‡ç­¾: {label_id} -> {label_name}")
        # è·³è¿‡èƒŒæ™¯æ ‡ç­¾
        if str(label_id) == '0' or label_name.lower() in ['background', 'èƒŒæ™¯']:
            print(f"  [è°ƒè¯•] è·³è¿‡èƒŒæ™¯æ ‡ç­¾: {label_id}")
            continue
        
        # æ ¹æ®å™¨å®˜åç§°è‡ªåŠ¨æ¨æ–­åˆé€‚çš„çª—å£è®¾ç½®
        window = _infer_window_from_organ_name(label_name)
        
        # æ¸…ç†å™¨å®˜åç§°ï¼ˆè½¬ä¸ºå°å†™ã€æ›¿æ¢ç©ºæ ¼ï¼‰
        clean_name = label_name.replace(' ', '_').replace('-', '_').lower()
        
        organ_mapping['organ_labels'][str(label_id)] = {
            'name': clean_name,
            'window': window,
            'original_name': label_name
        }
        print(f"  [è°ƒè¯•] æ·»åŠ å™¨å®˜: {label_id} -> {clean_name} (çª—å£: {window})")
    
    print(f"  æ„å»ºå™¨å®˜æ˜ å°„: {len(organ_mapping['organ_labels'])}ä¸ªå™¨å®˜")
    print(f"  [è°ƒè¯•] æœ€ç»ˆorgan_mapping['organ_labels']: {organ_mapping['organ_labels']}")
    
    return organ_mapping


def _infer_window_from_organ_name(organ_name: str) -> str:
    """
    æ ¹æ®å™¨å®˜åç§°è‡ªåŠ¨æ¨æ–­åˆé€‚çš„çª—å£è®¾ç½®
    
    Args:
        organ_name: å™¨å®˜åç§°
    
    Returns:
        çª—å£åç§°ï¼ˆlung, bone, soft_tissue, brainï¼‰
    """
    name_lower = organ_name.lower()
    
    # è‚ºçª— - è‚ºéƒ¨å’Œæ°”é“ç›¸å…³
    lung_keywords = ['lung', 'bronchus', 'bronchi', 'airway', 'trachea', 
                     'è‚º', 'æ”¯æ°”ç®¡', 'æ°”ç®¡']
    if any(kw in name_lower for kw in lung_keywords):
        return 'lung'
    
    # éª¨çª— - éª¨éª¼ç›¸å…³
    bone_keywords = ['bone', 'vertebra', 'vertebrae', 'rib', 'spine', 
                     'femur', 'tibia', 'humerus', 'skull', 'pelvis',
                     'éª¨', 'æ¤', 'è‚‹', 'è„ŠæŸ±', 'è‚¡éª¨', 'èƒ«éª¨', 'è‚±éª¨', 'é¢…éª¨', 'éª¨ç›†']
    if any(kw in name_lower for kw in bone_keywords):
        return 'bone'
    
    # è„‘çª— - è„‘éƒ¨ç›¸å…³
    brain_keywords = ['brain', 'cerebr', 'cerebellum', 'brainstem', 
                      'è„‘', 'å°è„‘', 'è„‘å¹²']
    if any(kw in name_lower for kw in brain_keywords):
        return 'brain'
    
    # é»˜è®¤ï¼šè½¯ç»„ç»‡çª— - é€‚ç”¨äºå¤§å¤šæ•°è…¹éƒ¨å™¨å®˜
    return 'soft_tissue'


def _process_m3d_seg_case_safe(case_info: Dict,
                               output_dir: str,
                               organ_mapping: Optional[Dict] = None,
                               default_resolution: int = DEFAULT_RESOLUTION,
                               compute_sdf: bool = False,
                               sdf_resolution: int = 512,
                               sdf_threshold_factor: float = 4.0,
                               replace_npy: bool = False,
                               use_mask: bool = False,
                               skip_existing: bool = True) -> Dict:
    """
    å®‰å…¨åŒ…è£…å‡½æ•°ï¼Œç”¨äºå¤šè¿›ç¨‹å¤„ç†æ—¶æ•è·è¯¦ç»†é”™è¯¯
    """
    debug_dir = os.path.join(output_dir, 'debug_logs')
    os.makedirs(debug_dir, exist_ok=True)
    debug_log_path = os.path.join(debug_dir, f"{case_info['case_id']}.log")
    fh = None
    try:
        import faulthandler
        fh = open(debug_log_path, 'a', encoding='utf-8')
        faulthandler.enable(file=fh, all_threads=True)
        return process_m3d_seg_case(
            case_info, output_dir, organ_mapping, default_resolution,
            compute_sdf, sdf_resolution, sdf_threshold_factor,
            replace_npy, use_mask, skip_existing,
            debug_dir=debug_dir
        )
    except Exception as e:
        import traceback
        error_msg = f"é”™è¯¯ç±»å‹: {type(e).__name__}\n"
        error_msg += f"é”™è¯¯ä¿¡æ¯: {str(e)}\n"
        error_msg += f"å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}"
        
        print(f"\n{'='*70}")
        print(f"âŒ å¤„ç†ç—…ä¾‹å¤±è´¥: {case_info['case_id']}")
        print(error_msg)
        print(f"{'='*70}")
        
        # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return {
            'case_id': case_info['case_id'],
            'error': str(e),
            'error_type': type(e).__name__,
            'processing_failed': True
        }
    finally:
        if fh is not None:
            try:
                fh.flush()
            finally:
                fh.close()


def process_m3d_seg_case(case_info: Dict,
                         output_dir: str,
                         organ_mapping: Optional[Dict] = None,
                         default_resolution: int = DEFAULT_RESOLUTION,
                         compute_sdf: bool = False,
                         sdf_resolution: int = 512,
                         sdf_threshold_factor: float = 4.0,
                         replace_npy: bool = False,
                         use_mask: bool = False,
                         skip_existing: bool = True,
                         debug_dir: Optional[str] = None) -> Dict:
    """
    å¤„ç†M3D-Segæ ¼å¼çš„å•ä¸ªç—…ä¾‹
    
    Args:
        case_info: ç—…ä¾‹ä¿¡æ¯
        output_dir: è¾“å‡ºç›®å½•
        organ_mapping: å™¨å®˜æ˜ å°„
        default_resolution: ç›®æ ‡åˆ†è¾¨ç‡
        compute_sdf: æ˜¯å¦è®¡ç®—SDF
        sdf_resolution: SDFåˆ†è¾¨ç‡
        sdf_threshold_factor: SDFé˜ˆå€¼å› å­
        replace_npy: æ˜¯å¦ç”¨NPZæ›¿æ¢NPYæ–‡ä»¶
        use_mask: æ˜¯å¦ä½¿ç”¨æ©ç æ¨¡å¼ï¼ˆè·³è¿‡çª—ä½çª—å®½å¤„ç†ï¼‰
        skip_existing: æ˜¯å¦è·³è¿‡å·²å¤„ç†çš„ç—…ä¾‹
    
    Returns:
        å¤„ç†ç»“æœä¿¡æ¯
    """
    case_id = case_info['case_id']
    case_dir = case_info['case_dir']
    debug_log_path = None

    def _debug_log(message: str) -> None:
        if debug_log_path is None:
            return
        ts = time.strftime('%Y-%m-%d %H:%M:%S')
        with open(debug_log_path, 'a', encoding='utf-8') as debug_fh:
            debug_fh.write(f"[{ts}] {message}\n")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    case_output_dir = os.path.join(output_dir, 'processed', case_id)
    if debug_dir:
        os.makedirs(debug_dir, exist_ok=True)
        debug_log_path = os.path.join(debug_dir, f"{case_id}.log")
        _debug_log(f"start case_id={case_id} pid={os.getpid()} case_dir={case_dir}")
    
    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆæ–­ç‚¹ç»­ä¼ åŠŸèƒ½ï¼‰
    if skip_existing:
        info_path = os.path.join(case_output_dir, 'info.json')
        if os.path.exists(info_path):
            try:
                skip_start = time.time()
                with open(info_path, 'r', encoding='utf-8') as f:
                    existing_info = json.load(f)
                skip_time = time.time() - skip_start
                
                # æ˜¾ç¤ºæ›´æ¸…æ™°çš„ä¿¡æ¯ï¼šåŒºåˆ†åŸå¤„ç†æ—¶é—´å’Œè·³è¿‡æ“ä½œæ—¶é—´
                original_time = existing_info.get('processing_time_sec', 0)
                print(f"\nâ­ï¸  è·³è¿‡å·²å¤„ç†: {case_id} | è·³è¿‡ç”¨æ—¶: {skip_time:.3f}ç§’ | èŠ‚çœ: {original_time:.2f}ç§’")
                
                # æ·»åŠ æ ‡è®°è¡¨ç¤ºè¿™æ˜¯è·³è¿‡çš„ç—…ä¾‹
                existing_info['_skipped'] = True
                return existing_info
            except Exception as e:
                print(f"\nâš ï¸  è­¦å‘Š: è¯»å–å·²æœ‰info.jsonå¤±è´¥: {e}ï¼Œå°†é‡æ–°å¤„ç†ç—…ä¾‹: {case_id}")
    
    print(f"\nå¤„ç†ç—…ä¾‹: {case_id}")
    start_time = time.time()
    
    os.makedirs(case_output_dir, exist_ok=True)
    if not use_mask:
        os.makedirs(os.path.join(case_output_dir, 'windows'), exist_ok=True)
    
    # æ­¥éª¤1: åŠ è½½æ•°æ®
    print(f"  1. åŠ è½½M3D-Segæ•°æ®...")
    ct_array, seg_array, mask_shape = load_m3d_seg_case(case_dir)
    
    # å¤„ç†CTæ•°ç»„ç»´åº¦
    if ct_array.ndim == 4:
        if ct_array.shape[0] == 1:
            ct_array = ct_array[0]
        else:
            print(f"     è­¦å‘Š: CTæœ‰å¤šä¸ªé€šé“ {ct_array.shape}ï¼Œåªä½¿ç”¨ç¬¬ä¸€ä¸ªé€šé“")
            ct_array = ct_array[0]
    
    original_shape = ct_array.shape
    print(f"     åŸå§‹å½¢çŠ¶: {original_shape}")
    print(f"     HUå€¼èŒƒå›´: [{np.min(ct_array):.2f}, {np.max(ct_array):.2f}]")
    
    # æ­¥éª¤2: åˆ†è¾¨ç‡é€‚é…
    print(f"  2. åˆ†è¾¨ç‡é€‚é…...")
    target_resolution = determine_target_resolution(original_shape, default_resolution)
    print(f"     ç›®æ ‡åˆ†è¾¨ç‡: {target_resolution}Â³")
    
    ct_adapted = adapt_resolution(ct_array, target_resolution)
    adapted_shape = ct_adapted.shape
    print(f"     é€‚é…åå½¢çŠ¶: {adapted_shape}")
    
    # å¦‚æœæœ‰åˆ†å‰²æ ‡ç­¾ï¼Œä¹Ÿè¿›è¡Œé€‚é…
    seg_adapted = None
    if seg_array is not None:
        # å¤„ç†åˆ†å‰²æ•°ç»„ç»´åº¦ï¼ˆ4D -> 3Dï¼‰
        if seg_array.ndim == 4:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ one-hot ç¼–ç 
            if seg_array.shape[0] <= 20:  # é€šé“åœ¨ç¬¬ä¸€ç»´
                sum_along_channel = seg_array.sum(axis=0)
                max_overlap = sum_along_channel.max()
                
                if max_overlap <= 1.1:
                    # one-hot ç¼–ç ï¼Œä½¿ç”¨ argmax è½¬æ¢
                    seg_array = np.argmax(seg_array, axis=0).astype(np.uint8)
                else:
                    # é one-hotï¼Œå°è¯• squeeze æˆ–å–ç¬¬ä¸€ä¸ªé€šé“
                    seg_array = seg_array.squeeze()
                    if seg_array.ndim == 4:
                        seg_array = seg_array[0]
            else:
                seg_array = seg_array.squeeze()
        
        seg_adapted = adapt_resolution(seg_array, target_resolution, fill_value=0, mode='constant')
        print(f"     åˆ†å‰²æ ‡ç­¾å·²é€‚é…")
    
    # æ ¹æ® use_mask å‚æ•°é€‰æ‹©ä¸åŒçš„å¤„ç†æµç¨‹
    organs_info = []
    global_windows = {}
    
    if use_mask:
        # ===== æ©ç æ¨¡å¼ï¼šç›´æ¥ä»åˆ†å‰²æ©ç æå–å„å™¨å®˜äºŒå€¼åŒ–ç½‘æ ¼ =====
        print(f"  3. ä½¿ç”¨æ©ç æ¨¡å¼ï¼ˆè·³è¿‡çª—ä½çª—å®½å¤„ç†ï¼‰...")
        
        # è°ƒè¯•ä¿¡æ¯
        print(f"     [è°ƒè¯•] seg_adapted is None: {seg_adapted is None}")
        print(f"     [è°ƒè¯•] organ_mapping is None: {organ_mapping is None}")
        if organ_mapping is not None:
            print(f"     [è°ƒè¯•] organ_mapping keys: {organ_mapping.keys()}")
            print(f"     [è°ƒè¯•] organ_labels: {organ_mapping.get('organ_labels', {})}")
        
        if seg_adapted is not None and organ_mapping is not None:
            organ_label_to_name = {}  # æ ‡ç­¾å€¼ -> å™¨å®˜åç§°çš„æ˜ å°„
            masks_dir = os.path.join(case_output_dir, 'masks')
            os.makedirs(masks_dir, exist_ok=True)
            
            # éå†æ‰€æœ‰å™¨å®˜
            organ_labels = organ_mapping.get('organ_labels', {})
            print(f"     [è°ƒè¯•] éå† {len(organ_labels)} ä¸ªå™¨å®˜æ ‡ç­¾")
            for label_str, organ_info in organ_labels.items():
                organ_label = int(label_str)
                organ_name = organ_info['name']
                
                # æå–å™¨å®˜æ©ç ï¼ˆäºŒå€¼åŒ–ï¼š1è¡¨ç¤ºå™¨å®˜ï¼Œ0è¡¨ç¤ºèƒŒæ™¯ï¼‰
                organ_binary = (seg_adapted == organ_label).astype(np.uint8)
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¯¥å™¨å®˜
                if organ_binary.sum() == 0:
                    continue
                
                # ä¿å­˜æ ‡ç­¾æ˜ å°„ï¼šæ ‡ç­¾å€¼ -> å™¨å®˜åç§°
                organ_label_to_name[str(organ_label)] = organ_name
                
                # ä½¿ç”¨æ ‡ç­¾å€¼ä½œä¸ºæ–‡ä»¶å
                binary_path = os.path.join(masks_dir, f'{organ_label}_binary.npy')
                np.save(binary_path, organ_binary)
                print(f"     ä¿å­˜ {organ_name} (æ ‡ç­¾{organ_label}): {int(organ_binary.sum())} ä½“ç´ ")
                
                # å¦‚æœéœ€è¦è®¡ç®—SDF
                if compute_sdf:
                    from ct_preprocessing.sdf_processor import convert_window_to_sdf, save_sdf_result
                    try:
                        _debug_log(f"  å¼€å§‹SDFè®¡ç®—: {organ_name} (æ ‡ç­¾{organ_label})")
                        _debug_log(f"    åŸå§‹shape={organ_binary.shape}, dtype={organ_binary.dtype}, sum={organ_binary.sum()}")
                        
                        # ç¡®ä¿ organ_binary æ˜¯ 3D
                        if organ_binary.ndim == 4:
                            if organ_binary.shape[0] == 1:
                                organ_binary = organ_binary[0]
                            elif 1 in organ_binary.shape:
                                organ_binary = organ_binary.squeeze()
                            else:
                                organ_binary = organ_binary[0]
                        
                        _debug_log(f"    å¤„ç†åshape={organ_binary.shape}")
                        
                        # æ•°æ®éªŒè¯
                        voxel_count = organ_binary.sum()
                        if voxel_count < 100:
                            _debug_log(f"    è·³è¿‡: ä½“ç´ æ•°å¤ªå°‘ ({voxel_count})")
                            print(f"       - è·³è¿‡SDF: ä½“ç´ æ•°å¤ªå°‘ ({voxel_count})")
                        else:
                            _debug_log(f"    å¼€å§‹convert_window_to_sdf (ä½“ç´ æ•°={voxel_count})")
                            
                            # æ£€æŸ¥CUDAå†…å­˜
                            import torch
                            if torch.cuda.is_available():
                                mem_allocated = torch.cuda.memory_allocated() / 1024**3
                                mem_reserved = torch.cuda.memory_reserved() / 1024**3
                                _debug_log(f"    CUDAå†…å­˜: allocated={mem_allocated:.2f}GB, reserved={mem_reserved:.2f}GB")
                            
                            sdf_result = convert_window_to_sdf(
                                organ_binary,
                                resolution=sdf_resolution,
                                threshold_factor=sdf_threshold_factor
                            )
                            _debug_log(f"    SDFè®¡ç®—å®Œæˆ")
                            
                            sdf_path = os.path.join(masks_dir, f'{organ_label}_sdf.npz')
                            save_sdf_result(
                                sdf_result,
                                sdf_path,
                                replace_source=replace_npy,
                                source_path=binary_path if replace_npy else None
                            )
                            sdf_points = len(sdf_result['sparse_index'])
                            _debug_log(f"    SDFå·²ä¿å­˜: {sdf_points}ç‚¹")
                            print(f"       - SDFç‚¹æ•°: {sdf_points}")
                            
                            # æ¸…ç†CUDAå†…å­˜
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                                _debug_log(f"    CUDAç¼“å­˜å·²æ¸…ç†")
                            
                    except Exception as e:
                        import traceback
                        error_trace = traceback.format_exc()
                        _debug_log(f"    SDFè®¡ç®—å¤±è´¥: {type(e).__name__}: {e}")
                        _debug_log(f"    é”™è¯¯å †æ ˆ:\n{error_trace}")
                        print(f"       - SDFè®¡ç®—å¤±è´¥: {type(e).__name__}: {e}")
                        print(f"       - è¯¦ç»†é”™è¯¯è§debugæ—¥å¿—: {debug_log_path}")
                
                # è®°å½•å™¨å®˜ä¿¡æ¯
                organs_info.append({
                    'name': organ_name,
                    'label': organ_label,
                    'voxel_count': int(organ_binary.sum())
                })
            
            # ä¿å­˜å™¨å®˜æ ‡ç­¾æ˜ å°„ä¿¡æ¯åˆ°JSONæ–‡ä»¶
            if organ_label_to_name:
                organ_labels_path = os.path.join(masks_dir, 'organ_labels.json')
                with open(organ_labels_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        'label_to_name': organ_label_to_name,
                        'dataset_name': organ_mapping.get('dataset_name', 'unknown'),
                        'modality': organ_mapping.get('modality', 'CT'),
                        'resolution': target_resolution,
                        'num_organs': len(organ_label_to_name),
                        'description': 'æ ‡ç­¾å€¼åˆ°å™¨å®˜åç§°çš„æ˜ å°„'
                    }, f, indent=2, ensure_ascii=False)
                print(f"     ä¿å­˜å™¨å®˜æ ‡ç­¾æ˜ å°„: masks/organ_labels.json")
            
            # ä¿å­˜å®Œæ•´çš„åˆ†å‰²æ©ç ï¼ˆç¨€ç–æ ¼å¼ï¼‰- å·²ç¦ç”¨ä»¥èŠ‚çœç©ºé—´
            # mask_shape_save = seg_adapted.shape
            # seg_flat = seg_adapted.reshape(-1)
            # seg_sparse = sparse.csr_matrix(seg_flat)
            # mask_path = os.path.join(masks_dir, 'segmentation_masks.npz')
            # sparse.save_npz(mask_path, seg_sparse)
            # print(f"     ä¿å­˜åˆ†å‰²æ©ç : masks/segmentation_masks.npz")
        else:
            print(f"  è­¦å‘Š: æ©ç æ¨¡å¼éœ€è¦åˆ†å‰²æ ‡ç­¾å’Œå™¨å®˜æ˜ å°„ï¼Œè·³è¿‡å¤„ç†")
    
    else:
        # ===== åŸæœ‰æµç¨‹ï¼šçª—ä½çª—å®½å¤„ç† =====
        # ä¿å­˜åŸå§‹é€‚é…åçš„CT
        ct_original_path = os.path.join(case_output_dir, f'ct_original_{target_resolution}.npy')
        np.save(ct_original_path, ct_adapted)
        print(f"     ä¿å­˜åŸå§‹CT: ct_original_{target_resolution}.npy")
        
        # æ­¥éª¤3: å…¨å±€çª—å£å¤„ç†ï¼ˆç›´æ¥åœ¨åŸå§‹CTä¸Šè¿›è¡ŒäºŒå€¼åŒ–ï¼‰
        print(f"  3. å…¨å±€çª—å£å¤„ç†ï¼ˆåŸºäºåŸå§‹HUå€¼ï¼‰...")
        if compute_sdf:
            print(f"     - åŒæ—¶è®¡ç®—SDF (åˆ†è¾¨ç‡={sdf_resolution}, é˜ˆå€¼å› å­={sdf_threshold_factor})")
        
        global_windows = process_all_windows(
            ct_adapted, 
            binarize=True,
            compute_sdf=compute_sdf,
            sdf_resolution=sdf_resolution,
            sdf_threshold_factor=sdf_threshold_factor
        )
        
        # ä¿å­˜çª—å£ç»“æœ
        from ct_preprocessing.window_processor import save_window_results
        windows_dir = os.path.join(case_output_dir, 'windows')
        saved_paths = save_window_results(global_windows, windows_dir, replace_npy=replace_npy)
        
        for window_name, result in global_windows.items():
            if isinstance(result, dict) and 'binary' in result:
                binary_array = result['binary']
                sdf_points = len(result['sdf']['sparse_index']) if 'sdf' in result else 0
                positive_ratio = np.sum(binary_array) / binary_array.size
                print(f"     {window_name}: {positive_ratio:.2%} æ­£å€¼, SDFç‚¹æ•°: {sdf_points}")
            else:
                binary_array = result
                positive_ratio = np.sum(binary_array) / binary_array.size
                print(f"     {window_name}: {positive_ratio:.2%} æ­£å€¼")
        
        # æ­¥éª¤4: å™¨å®˜ç‰¹å®šçª—å£å¤„ç†
        if seg_adapted is not None and organ_mapping is not None:
            print(f"  4. å™¨å®˜ç‰¹å®šçª—å£å¤„ç†...")
            
            # éªŒè¯åˆ†å‰²
            is_valid, message = validate_segmentation(seg_adapted, ct_adapted)
            if not is_valid:
                print(f"     è­¦å‘Š: {message}")
            else:
                print(f"     {message}")
            
            # è·å–å­˜åœ¨çš„å™¨å®˜
            organs_present = get_organs_present(seg_adapted, organ_mapping)
            print(f"     å‘ç° {len(organs_present)} ä¸ªå™¨å®˜")
            
            # å¤„ç†æ‰€æœ‰å™¨å®˜
            organ_results = process_all_organs(
                ct_adapted,
                seg_adapted,
                organ_mapping,
                save_global_windows=False,
                compute_sdf=compute_sdf,
                sdf_resolution=sdf_resolution,
                sdf_threshold_factor=sdf_threshold_factor
            )
            
            # ä¿å­˜å™¨å®˜æ•°æ®
            for organ_name, organ_data in organ_results['organs'].items():
                organ_dir = os.path.join(case_output_dir, 'organs', organ_name)
                os.makedirs(organ_dir, exist_ok=True)
                
                for window_filename, window_result in organ_data.items():
                    if window_filename in ['mask', 'label', 'window_used']:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«SDFç»“æœ
                    if isinstance(window_result, dict) and 'sdf' in window_result:
                        # ä¿å­˜äºŒå€¼åŒ–ç»“æœ
                        npy_path = os.path.join(organ_dir, window_filename + '.npy')
                        np.save(npy_path, window_result['binary'])
                        
                        # ä¿å­˜SDFç»“æœ
                        from ct_preprocessing.sdf_processor import save_sdf_result
                        npz_path = os.path.join(organ_dir, window_filename + '.npz')
                        save_sdf_result(
                            window_result['sdf'],
                            npz_path,
                            replace_source=replace_npy,
                            source_path=npy_path if replace_npy else None
                        )
                    else:
                        # åªæœ‰äºŒå€¼åŒ–ç»“æœ
                        window_path = os.path.join(organ_dir, window_filename + '.npy')
                        np.save(window_path, window_result)
                
                # ç»Ÿè®¡
                organ_label = organ_data['label']
                organ_stats = compute_organ_statistics(ct_adapted, seg_adapted, organ_label)
                organs_info.append({
                    'name': organ_name,
                    'label': organ_label,
                    'window': organ_data['window_used'],
                    'voxel_count': organ_stats['voxel_count'],
                    'hu_mean': organ_stats['hu_mean'],
                    'hu_std': organ_stats['hu_std']
                })
                
                print(f"       {organ_name}: {organ_stats['voxel_count']} ä½“ç´ ")
            
            # ä¿å­˜åˆ†å‰²æ©ç  - å·²ç¦ç”¨ä»¥èŠ‚çœç©ºé—´
            # masks_dir = os.path.join(case_output_dir, 'masks')
            # os.makedirs(masks_dir, exist_ok=True)
            # 
            # mask_shape_save = seg_adapted.shape
            # seg_flat = seg_adapted.reshape(-1)
            # seg_sparse = sparse.csr_matrix(seg_flat)
            # mask_path = os.path.join(masks_dir, 'segmentation_masks.npz')
            # sparse.save_npz(mask_path, seg_sparse)
            # print(f"     ä¿å­˜åˆ†å‰²æ©ç ")
        else:
            print(f"  4. è·³è¿‡å™¨å®˜å¤„ç†")
    
    # ç”Ÿæˆå…ƒä¿¡æ¯
    processing_time = time.time() - start_time
    
    total_size = 0
    for root, dirs, files in os.walk(case_output_dir):
        for file in files:
            total_size += os.path.getsize(os.path.join(root, file))
    
    file_size_mb = total_size / (1024 * 1024)
    
    info = {
        'case_id': case_id,
        'original_shape': list(original_shape),
        'adapted_shape': list(adapted_shape),
        'resolution': target_resolution,
        'has_segmentation': seg_array is not None,
        'organs_present': organs_info,
        'windows_processed': list(global_windows.keys()),
        'file_size_mb': round(file_size_mb, 2),
        'processing_time_sec': round(processing_time, 2),
        'use_mask': use_mask,
        'ct_path': f'processed/{case_id}/ct_original_{target_resolution}.npy' if not use_mask else None,
        'masks_path': None,  # segmentation_masks.npz ä¸å†ä¿å­˜
        'organ_labels_file': f'processed/{case_id}/masks/organ_labels.json' if use_mask and organs_info else None,
        'source_format': 'm3d_seg'
    }
    
    info_path = os.path.join(case_output_dir, 'info.json')
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2, ensure_ascii=False)
    
    print(f"  å®Œæˆï¼è€—æ—¶: {processing_time:.2f}ç§’")
    
    return info


def scan_m3d_seg_dataset(dataset_root: str) -> tuple:
    """
    æ‰«æM3D-Segæ ¼å¼çš„æ•°æ®é›†
    
    Args:
        dataset_root: æ•°æ®é›†æ ¹ç›®å½•
    
    Returns:
        (case_list, dataset_json)
    """
    # æŸ¥æ‰¾JSONæ–‡ä»¶
    json_files = glob.glob(os.path.join(dataset_root, '*.json'))
    
    dataset_json = None
    if json_files:
        json_path = json_files[0]
        dataset_json = load_dataset_json(json_path)
        print(f"  å‘ç°æ•°æ®é›†é…ç½®: {json_path}")
    
    # æ‰«æå­æ–‡ä»¶å¤¹
    case_list = []
    for item in os.listdir(dataset_root):
        item_path = os.path.join(dataset_root, item)
        if os.path.isdir(item_path):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«image.npy
            image_path = os.path.join(item_path, 'image.npy')
            if os.path.exists(image_path):
                case_list.append({
                    'case_id': item,
                    'case_dir': item_path
                })
    
    return case_list, dataset_json


def process_m3d_seg_dataset(dataset_root: str,
                            output_dir: str,
                            default_resolution: int = DEFAULT_RESOLUTION,
                            num_workers: int = 1,
                            compute_sdf: bool = False,
                            sdf_resolution: int = 512,
                            sdf_threshold_factor: float = 4.0,
                            replace_npy: bool = False,
                            use_mask: bool = False,
                            skip_existing: bool = True) -> pd.DataFrame:
    """
    å¤„ç†å®Œæ•´çš„M3D-Segæ•°æ®é›†
    
    Args:
        dataset_root: M3D-Segæ•°æ®é›†æ ¹ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        default_resolution: é»˜è®¤åˆ†è¾¨ç‡
        num_workers: å¹¶è¡Œè¿›ç¨‹æ•°
        compute_sdf: æ˜¯å¦è®¡ç®—SDF
        sdf_resolution: SDFåˆ†è¾¨ç‡
        sdf_threshold_factor: SDFé˜ˆå€¼å› å­
        replace_npy: æ˜¯å¦ç”¨NPZæ›¿æ¢NPYæ–‡ä»¶
        use_mask: æ˜¯å¦ä½¿ç”¨æ©ç æ¨¡å¼ï¼ˆè·³è¿‡çª—ä½çª—å®½å¤„ç†ï¼‰
        skip_existing: æ˜¯å¦è·³è¿‡å·²å¤„ç†çš„ç—…ä¾‹ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    
    Returns:
        å…ƒæ•°æ®DataFrame
    """
    print("=" * 70)
    print("M3D-Segæ ¼å¼æ•°æ®é¢„å¤„ç†")
    print("=" * 70)
    
    # æ£€æŸ¥SDFä¾èµ–
    if compute_sdf:
        from ct_preprocessing import check_cuda_available, check_trellis_available
        if not check_cuda_available():
            print("\nâš ï¸  è­¦å‘Š: CUDAä¸å¯ç”¨ï¼ŒSDFè®¡ç®—éœ€è¦GPUæ”¯æŒ")
            compute_sdf = False
        # elif not check_trellis_available():
        #     print("\nâš ï¸  è­¦å‘Š: TRELLISä¸å¯ç”¨ï¼Œè·³è¿‡SDFè®¡ç®—")
        #     compute_sdf = False
        else:
            print(f"\nâœ“ SDFè®¡ç®—å·²å¯ç”¨ (åˆ†è¾¨ç‡={sdf_resolution}, æ›¿æ¢NPY={replace_npy})")
    
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, 'processed'), exist_ok=True)
    
    # æ‰«ææ•°æ®é›†
    print(f"\næ‰«æM3D-Segæ•°æ®é›†: {dataset_root}")
    case_list, dataset_json = scan_m3d_seg_dataset(dataset_root)
    print(f"  å‘ç° {len(case_list)} ä¸ªç—…ä¾‹")
    
    # æ„å»ºå™¨å®˜æ˜ å°„ï¼ˆç›´æ¥ä»æ•°æ®é›†JSONä¸­è¯»å–ï¼Œæ— éœ€é¢å¤–çš„organ_labels.jsonï¼‰
    organ_mapping = None
    if dataset_json:
        organ_mapping = build_organ_mapping_from_json(dataset_json, dataset_root)
        print(f"  æ•°æ®é›†: {organ_mapping['dataset_name']}")
        print(f"  å™¨å®˜æ•°: {len(organ_mapping['organ_labels'])}")
    
    # æ£€æŸ¥å·²å¤„ç†çš„ç—…ä¾‹æ•°é‡ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ç»Ÿè®¡ï¼‰
    if skip_existing:
        existing_count = 0
        for case_info in case_list:
            info_path = os.path.join(output_dir, 'processed', case_info['case_id'], 'info.json')
            if os.path.exists(info_path):
                existing_count += 1
        
        if existing_count > 0:
            print(f"\nâœ“ æ–­ç‚¹ç»­ä¼ : å‘ç° {existing_count} ä¸ªå·²å¤„ç†ç—…ä¾‹ï¼Œå°†è·³è¿‡")
            print(f"  å¾…å¤„ç†: {len(case_list) - existing_count} ä¸ªç—…ä¾‹")
    
    # å¤„ç†æ‰€æœ‰ç—…ä¾‹
    print(f"\nå¼€å§‹å¤„ç†ï¼ˆå¹¶è¡Œè¿›ç¨‹æ•°: {num_workers}ï¼‰...")
    
    # å†…å­˜ä½¿ç”¨è­¦å‘Š
    if num_workers > 8:
        print(f"\nâš ï¸  è­¦å‘Š: å¹¶è¡Œè¿›ç¨‹æ•°è¾ƒé«˜ ({num_workers})ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜ä¸è¶³")
        print(f"   å»ºè®®: å‡å°‘åˆ° 4-8 ä¸ªè¿›ç¨‹ä»¥é¿å…è¿›ç¨‹å´©æºƒ")
    
    print("=" * 70)
    
    metadata_list = []
    failed_cases = []
    
    if num_workers == 1:
        for case_info in case_list:
            try:
                info = process_m3d_seg_case(
                    case_info,
                    output_dir,
                    organ_mapping,
                    default_resolution,
                    compute_sdf,
                    sdf_resolution,
                    sdf_threshold_factor,
                    replace_npy,
                    use_mask,
                    skip_existing
                )
                metadata_list.append(info)
            except Exception as e:
                import traceback
                error_msg = str(e)
                failed_cases.append({
                    'case_id': case_info['case_id'],
                    'error': error_msg,
                    'traceback': traceback.format_exc()
                })
                print(f"  âŒ é”™è¯¯: {case_info['case_id']}: {error_msg}")
    else:
        with ProcessPoolExecutor(max_workers=num_workers) as executor:
            futures = []
            for case_info in case_list:
                future = executor.submit(
                    _process_m3d_seg_case_safe,  # ä½¿ç”¨å®‰å…¨åŒ…è£…å‡½æ•°
                    case_info,
                    output_dir,
                    organ_mapping,
                    default_resolution,
                    compute_sdf,
                    sdf_resolution,
                    sdf_threshold_factor,
                    replace_npy,
                    use_mask,
                    skip_existing
                )
                futures.append((future, case_info['case_id']))
            
            failed_cases = []
            for future, case_id in tqdm(futures, desc="å¤„ç†è¿›åº¦"):
                try:
                    # æ·»åŠ è¶…æ—¶æœºåˆ¶ï¼ˆæ¯ä¸ªç—…ä¾‹æœ€å¤š30åˆ†é’Ÿï¼‰
                    info = future.result(timeout=1800)
                    
                    # æ£€æŸ¥æ˜¯å¦å¤„ç†å¤±è´¥
                    if info.get('processing_failed', False):
                        failed_cases.append({
                            'case_id': case_id,
                            'error': info.get('error', 'Unknown error')
                        })
                        print(f"\n  âš ï¸  ç—…ä¾‹å¤„ç†å¤±è´¥ï¼Œå·²è®°å½•: {case_id}")
                    else:
                        metadata_list.append(info)
                        
                except TimeoutError:
                    error_msg = f"å¤„ç†è¶…æ—¶ï¼ˆ>30åˆ†é’Ÿï¼‰"
                    failed_cases.append({
                        'case_id': case_id,
                        'error': error_msg
                    })
                    print(f"\n  â±ï¸  è¶…æ—¶: {case_id}: {error_msg}")
                except Exception as e:
                    error_msg = f"{type(e).__name__}: {str(e)}"
                    failed_cases.append({
                        'case_id': case_id,
                        'error': error_msg
                    })
                    print(f"\n  âŒ é”™è¯¯: {case_id}: {error_msg}")
    
    # ç”Ÿæˆå…ƒæ•°æ®
    print("\n" + "=" * 70)
    print("ç”Ÿæˆå…ƒæ•°æ®...")
    
    # ç»Ÿè®¡è·³è¿‡å’Œå¤„ç†çš„ç—…ä¾‹
    skipped_count = sum(1 for info in metadata_list if info.get('_skipped', False))
    processed_count = len(metadata_list) - skipped_count
    
    if skip_existing and skipped_count > 0:
        print(f"  âœ“ è·³è¿‡å·²å¤„ç†: {skipped_count} ä¸ªç—…ä¾‹")
        print(f"  âœ“ æ–°å¤„ç†: {processed_count} ä¸ªç—…ä¾‹")
    
    # æŠ¥å‘Šå¤±è´¥çš„ç—…ä¾‹
    if failed_cases:
        print(f"\n  âš ï¸  å¤„ç†å¤±è´¥: {len(failed_cases)} ä¸ªç—…ä¾‹")
        
        # ä¿å­˜å¤±è´¥ç—…ä¾‹åˆ—è¡¨
        failed_log_path = os.path.join(output_dir, 'failed_cases.json')
        with open(failed_log_path, 'w', encoding='utf-8') as f:
            json.dump({
                'total_failed': len(failed_cases),
                'failed_cases': failed_cases,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }, f, indent=2, ensure_ascii=False)
        
        print(f"  å¤±è´¥ç—…ä¾‹è¯¦æƒ…å·²ä¿å­˜: failed_cases.json")
        
        if num_workers > 1:
            print(f"\n  ğŸ’¡ å»ºè®®ï¼š")
            print(f"    1. æ£€æŸ¥å¤±è´¥ç—…ä¾‹çš„æ•°æ®æ˜¯å¦æŸå")
            print(f"    2. å°è¯•å‡å°‘å¹¶è¡Œè¿›ç¨‹æ•° (å½“å‰: {num_workers}ï¼Œå»ºè®®: 4-8)")
            print(f"    3. ä½¿ç”¨ --num_workers 1 å•ç‹¬å¤„ç†å¤±è´¥çš„ç—…ä¾‹")
    
    # æ¸…ç†ä¸´æ—¶æ ‡è®°
    for info in metadata_list:
        info.pop('_skipped', None)
    
    metadata_df = pd.DataFrame(metadata_list)
    
    # è½¬æ¢å­—æ®µ
    if 'organs_present' in metadata_df.columns:
        metadata_df['organs_present'] = metadata_df['organs_present'].apply(json.dumps)
    if 'windows_processed' in metadata_df.columns:
        metadata_df['windows_processed'] = metadata_df['windows_processed'].apply(json.dumps)
    if 'original_shape' in metadata_df.columns:
        metadata_df['original_shape'] = metadata_df['original_shape'].apply(
            lambda x: ','.join(map(str, x))
        )
    if 'adapted_shape' in metadata_df.columns:
        metadata_df['adapted_shape'] = metadata_df['adapted_shape'].apply(
            lambda x: ','.join(map(str, x))
        )
    
    metadata_path = os.path.join(output_dir, 'metadata.csv')
    metadata_df.to_csv(metadata_path, index=False)
    print(f"  å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
    
    # æ•°æ®é›†é…ç½®
    dataset_config = {
        'dataset_name': organ_mapping['dataset_name'] if organ_mapping else 'Unknown',
        'modality': 'CT',
        'source_format': 'm3d_seg',
        'num_cases': len(metadata_df),
        'default_resolution': default_resolution,
        'processing_date': time.strftime('%Y-%m-%d %H:%M:%S'),
    }
    
    config_path = os.path.join(output_dir, 'dataset_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(dataset_config, f, indent=2, ensure_ascii=False)
    
    print("\nå¤„ç†å®Œæˆ!")
    print(f"  æ€»ç—…ä¾‹æ•°: {len(metadata_df)}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 70)
    
    return metadata_df


def main():
    parser = argparse.ArgumentParser(description='M3D-Segæ ¼å¼æ•°æ®é¢„å¤„ç†')
    
    parser.add_argument('--data_root', type=str, required=True,
                       help='M3D-Segæ•°æ®é›†æ ¹ç›®å½•ï¼ˆåŒ…å«å­æ–‡ä»¶å¤¹å’ŒJSONæ–‡ä»¶ï¼‰')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--default_resolution', type=int, default=DEFAULT_RESOLUTION,
                       help='é»˜è®¤ç›®æ ‡åˆ†è¾¨ç‡')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='å¹¶è¡Œè¿›ç¨‹æ•°')
    parser.add_argument('--compute_sdf', action='store_true',
                       help='è®¡ç®—çª—å£æ•°æ®çš„SDFè¡¨ç¤ºï¼ˆéœ€è¦CUDAå’ŒTRELLISï¼‰')
    parser.add_argument('--sdf_resolution', type=int, default=512,
                       help='SDFç›®æ ‡åˆ†è¾¨ç‡ï¼ˆé»˜è®¤: 512ï¼‰')
    parser.add_argument('--sdf_threshold_factor', type=float, default=4.0,
                       help='SDFé˜ˆå€¼å› å­ï¼ˆé»˜è®¤: 4.0ï¼‰')
    parser.add_argument('--replace_npy', action='store_true',
                       help='ç”¨NPZæ–‡ä»¶æ›¿æ¢åŸNPYæ–‡ä»¶')
    parser.add_argument('--use_mask', action='store_true',
                       help='ç›´æ¥ä½¿ç”¨åˆ†å‰²æ©ç ç”ŸæˆäºŒå€¼åŒ–ä½“ç´ ç½‘æ ¼ï¼Œè·³è¿‡çª—ä½çª—å®½å¤„ç†')
    parser.add_argument('--no_skip', action='store_true',
                       help='ä¸è·³è¿‡å·²å¤„ç†çš„ç—…ä¾‹ï¼Œå¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰ç—…ä¾‹')
    
    args = parser.parse_args()
    
    metadata_df = process_m3d_seg_dataset(
        dataset_root=args.data_root,
        output_dir=args.output_dir,
        default_resolution=args.default_resolution,
        num_workers=args.num_workers,
        compute_sdf=args.compute_sdf,
        sdf_resolution=args.sdf_resolution,
        sdf_threshold_factor=args.sdf_threshold_factor,
        replace_npy=args.replace_npy,
        use_mask=args.use_mask,
        skip_existing=not args.no_skip
    )
    
    print("\nå…¨éƒ¨å®Œæˆï¼")


if __name__ == '__main__':
    main()

